{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos usar o poder de Deep Learning para realizar tarefas de NLP? Bom, podemos pensar em aplicações mais complexas como _Speech Recognition_; _Chatbots_ que entendem perguntas complexas, entre outras situações. Contudo, podemos tirar proveito de NLP com Deep Learning de uma forma \"relativamente\" (áspas intencionais) simples: que é através do _Word2Vec_.\n",
    "\n",
    "O [_Word2Vec_](https://code.google.com/archive/p/word2vec/) é uma aplicação desenvolvida pela Google por volta de 2013 que nada mais é do que um modelo de _Embedding de palavras_. Para aqueles que queiram uma definição mais formal, essa [apresentação](https://docs.google.com/file/d/0B7XkCwpI5KDYRWRnd1RzWXQ2TWc/edit) pode ser util.\n",
    "\n",
    "## Embeddings de Palavras\n",
    "\n",
    "Quando lidamos com palavras em texto, podemos ter uma situação em que temos milhares de classes para prever, uma para cada palavra. Uma abordagem que podemos usar é a conhecida como **one hot encoding**, em que o número de neurônios da nossa Rede Neural equivale ao número de palavras do nosso _dataset_ e ao entrar diferentes palavras, sua posição no vocabulário é setada como 1 e nos demais, 0. Contudo, esse tipo de situação é extreamente ineficiente, já que por exemplo, podemos ter uma situação com apenas um elemento setado como 1 e 50,000 como 0. Computacionalmente é um processo caro.\n",
    "\n",
    "![One Hot Encoding](img/one-hot.png)\n",
    "\n",
    "Para resolver esse problema nós usamos o que pode ser chamado de _Word Embedding_._Embeddings_ nada mais são do que uma camada oculta (a camada de _Embedding_) de uma Rede Neural Artificial como essa que a gente acabou de ver. Nos _Embeddings_, não é preciso computar todos os neurônios, mas apenas pegar a linha dos vetores de peso (_embedding weights_). A gente pode fazer isso porquê a multiplicação de um vetor _one hot encoding_ por essa camada irá resultar apenas no valor da linha correspondente ao 1.\n",
    "\n",
    "![One Hot Multiply](img/one-multiply.png)\n",
    "\n",
    "\n",
    "Logo, quando tratamos com _Embeddings_, computacionalmente falando não ocorre multiplicação matricial, mas apenas um _lookup_. Lembra que os computadores entendem apenas números? Então, suponha que a palavra _house_ tenha sido encodada na linha 958 enquanto _life_ na 1056. Para obter os valores de peso de _house_ basta olhar na linha 958 da tabela de pesos. Esse processo é conhecido como **_embedding lookup_** e o número de neurônios dessa camada oculta é conhecida como **_embedding dimension_**.\n",
    "\n",
    "Embeddings não são usados para palavras, apenas. Você pode usá-los para qualquer modelo que tenha um número massivo de classe. Um desses modelos é o _Word2Vec_, que usa modelos de palavras para encontrar representações semânticas das palavras.\n",
    "\n",
    "## A intuição do Word2Vec\n",
    "\n",
    "O _Word2Vec_ foi uma aplicação capaz de mostrar que redes neurais conseguem encontrar assimilações entre palavras. Por exemplo:\n",
    "- **homem** está para **mulher** assim como **rei** está para?\n",
    "\n",
    "### Para pensar um pouco:\n",
    "\n",
    "Em seu paper original, Mikolov usou o dataset do _Google News_ que era composto de 3 milhões de palavras e frases, resultando em mais de 100 bilhões de palavras. Nele, foi possível encontrar associações bem interessantes, como:\n",
    "\n",
    "- Microsoft - Windows ; Google- ?; IBM- ?; Apple -?\n",
    "\n",
    "![Word2Vec](img/word2vec.png)\n",
    "\n",
    "### Por debaixo dos panos\n",
    "\n",
    "![SemanticSpace](img/semanticspace.png)\n",
    "\n",
    "\n",
    "Sendo assim, é possível encontrar relações bem interessantes, não só limitadas a relações pessoais, como também de verbos e lugares:\n",
    "\n",
    "\n",
    "![Relationships](img/relationships.png)\n",
    "\n",
    "\n",
    "### A arquitetura por trás disso\n",
    "\n",
    "A ideia aqui não é ensinar a como implementar o _Word2Vec_ do zero, mas sim entender toda a intuição por trás dele. Lembra que mais cedo eu falei que estruturas de _Embedding_ nada mais são do que camadas ocultas de Redes Neurais? Então, as duas arquiteturas que implementam o _Word2Vec_ são **Redes Neurais**: o continuous Bag of Words (**CBOW**) e o **Skip-gram**.\n",
    "\n",
    "![Architetures](img/arc.png)\n",
    "\n",
    "Como podemos ver, as diferenças entre cada uma das arquiteturas é a seguinte:\n",
    "\n",
    "-**CBOW**: Dado um contexto (uma janela de palavras), eu tento prever a palavra do meio da janela.\n",
    "\n",
    "-**Skip-gram**: Dada uma palavra, eu tento prever as palavras em volta (ou o contexto) em que ela está inserida.\n",
    "\n",
    "Pelo paper, a arquitetura _skipgram_ tende a performar melhor que o _CBOW_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando os dados\n",
    "\n",
    "Para a aula de hoje, usaremos apenas o pacote de processamento de texto `gensim`. A ideia é mostrar que ele é uma alternativa ao `scikit-learn` para algumas etapas de pré processamento, mas também é válido dizer que ambos são complementares. Em outras palavras, é possível usar a saída de algum processo do `gensim` no `scikit` e vice versa.\n",
    "\n",
    "Uma coisa legal do Word2Vec é que ele pode ser considerado como um tipo de aprendizado **não** surpervisionado. Em outras palavras, ele é capaz de aprender de textos que não possuam um rótulo (ou label). Para ilustrar isso, para o treinamento aqui, além de usarmos o _labeledTrainData.tsv_, também usaremos o _unlabeledTrainData.tsv_, que contém 50 mil reviews adicionais, sem nenhuma label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Faca a leitura dos arquivos labeledTrainData.tsv;testData.tsv e unlabeledTrainData.tsv\n",
    "#Armazene seus valores nas variáveis train, test e unlabeled_train respectivamente\n",
    "\n",
    "#Lembre-se de importar o pandas !\n",
    "train = \n",
    "test =\n",
    "unlabeled_train = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré Processando o texto\n",
    "\n",
    "É interessante realizarmos o pré processamento dos textos que vamos trabalhar, igual fizemos na aula passada. Contudo, diferente do modelo **BoW** que vimos, o _Word2Vec_ **leva** em consideração o contexto das palavras. Sendo assim, talvez não seja muito legal removermos as *stop words*, assim como reduzir as palavras ao seu radical (stemming ou lemma) pode não ser desejável.\n",
    "\n",
    "Contudo, um pré processamento interessante é remover as URL's, uma vez que a chance de um site ser citado em um review é baixo e não estamos muito interessados na relação entre sites, mas no contexto em que levou a pessoa a citar alguma fonte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    \n",
    "    # 1. Ignore URLs\n",
    "    review_text = re.sub(\"IMPLEMENTE AQUI A REGEX QUE IGNORA URL\",\"<URL>\", review)\n",
    "    # 2. Remove HTML\n",
    "    review_text = #IMPLEMENTE AQUI A FUNCAO QUE REMOVE OS <br> E OUTRAS COISAS DO HTML\n",
    "    #  \n",
    "    # 3. Remove non-letters\n",
    "    # IMPLEMENTE AQUI A FUNCAO QUE REMOVE CARACTER ESPECIAL\n",
    "    #\n",
    "    # 4. Convert words to lower case and split them\n",
    "    words = #IMPLEMENTE AQUI A FUNCAO QUE REMOVE EXCESSO DE ESPACOS, QUE CONVERTE TUDO\n",
    "            #PARA MINUSCULA E SPLITA AS PALAVRAS\n",
    "    #\n",
    "    # 5. Optionally remove stop words (false by default)\n",
    "    #INCLUA AQUI A PARTE DE REMOVER STOP WORDS\n",
    "    #LEMBRE-SE QUE AGORA ELA EH UMA BOOLEAN E NAO DEVE SER EXECUTADA\n",
    "    #SE SEU VALOR FOR FALSO\n",
    "    #\n",
    "    # 6. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez limpa, agora precisamos converter os nossos dados para o tipo de dado que a biblioteca do `gensim` aceite. O _Word2Vec_ recebe como entrada sentenças únicas, cada uma formada por uma lista de palavras. Ou seja, precisamos de uma lista de listas.\n",
    "\n",
    "Definir onde começa ou onde acaba uma sentença não é algo muito claro. Existem sentenças que podem terminar com \"?\";\".\";\"!\" entre outras. Por conta disso, usaremos o pacote sent_tokenize do NLTK para fazer esse tipo de split pra gente. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = 'O André roubou pão na casa do João ! Quem, Eu? Você ! Eu não. Então quem foi?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O André roubou pão na casa do João !',\n",
       " 'Quem, Eu?',\n",
       " 'Você !',\n",
       " 'Eu não.',\n",
       " 'Então quem foi?']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review,remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['o', 'andré', 'roubou', 'pão', 'na', 'casa', 'do', 'joão'],\n",
       " ['quem', 'eu'],\n",
       " ['você'],\n",
       " ['eu', 'não'],\n",
       " ['então', 'quem', 'foi']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_to_sentences(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 218/25000 [00:04<09:20, 44.21it/s]/Users/abarbosa/miniconda3/envs/somostera/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "100%|██████████| 25000/25000 [06:49<00:00, 61.04it/s]  \n",
      "  0%|          | 10/50000 [00:00<08:32, 97.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Unlabeled Sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31189/50000 [09:00<06:18, 49.71it/s]/Users/abarbosa/miniconda3/envs/somostera/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "100%|██████████| 50000/50000 [14:25<00:00, 57.80it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Inicialize uma lista de sentenca e inclua nela \n",
    "                #todos os reviews processados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, [5, 6, 7, 8]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = [1,2,3,4]\n",
    "brray = [5,6,7,8]\n",
    "\n",
    "array.append(brray)\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = [1,2,3,4]\n",
    "array += brray\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = [1,2,3,4]\n",
    "array.extend(brray)\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "795538"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with',\n",
       " 'all',\n",
       " 'this',\n",
       " 'stuff',\n",
       " 'going',\n",
       " 'down',\n",
       " 'at',\n",
       " 'the',\n",
       " 'moment',\n",
       " 'with',\n",
       " 'mj',\n",
       " 'i',\n",
       " 've',\n",
       " 'started',\n",
       " 'listening',\n",
       " 'to',\n",
       " 'his',\n",
       " 'music',\n",
       " 'watching',\n",
       " 'the',\n",
       " 'odd',\n",
       " 'documentary',\n",
       " 'here',\n",
       " 'and',\n",
       " 'there',\n",
       " 'watched',\n",
       " 'the',\n",
       " 'wiz',\n",
       " 'and',\n",
       " 'watched',\n",
       " 'moonwalker',\n",
       " 'again']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['maybe',\n",
       " 'i',\n",
       " 'just',\n",
       " 'want',\n",
       " 'to',\n",
       " 'get',\n",
       " 'a',\n",
       " 'certain',\n",
       " 'insight',\n",
       " 'into',\n",
       " 'this',\n",
       " 'guy',\n",
       " 'who',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'was',\n",
       " 'really',\n",
       " 'cool',\n",
       " 'in',\n",
       " 'the',\n",
       " 'eighties',\n",
       " 'just',\n",
       " 'to',\n",
       " 'maybe',\n",
       " 'make',\n",
       " 'up',\n",
       " 'my',\n",
       " 'mind',\n",
       " 'whether',\n",
       " 'he',\n",
       " 'is',\n",
       " 'guilty',\n",
       " 'or',\n",
       " 'innocent']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o modelo Word2Vec\n",
    "\n",
    "Com a lista de sentenças devidamente parseada, basta treinarmos o nosso modelo. Aqui tem uma lista dos parâmetros especificando o que cada uma coisa faz.\n",
    "\n",
    "- **Architecture**: As opções de arquitetura são os algoritmos possíveis para a realização do Word2Vec. As opções aqui são o _skip-gram_ (default) e o _continuous bag of words_ (_cbow_).\n",
    "\n",
    "- **Training algorithm**: _Hierarchical softmax_ (default) ou _negative sampling_. Aqui, o default funcionou bem.\n",
    "\n",
    "- **Downsampling of frequent words**: A documentação do Google sugere valores entre .00001 and .001. Aqui, algo epor volta de 0.001 parece que melhorou o modelo final.\n",
    "\n",
    "- **Word vector dimensionality**: Aqui a ideia de _feature_ é parecida com o modelo que vimos do _BoW_, mas cada coluna não necessariamente é uma palavra. Logo, mais _features_ podem resultar em tempos maiores de processamento, mas nem sempre em modelos ideiais. O default aqui é 300.\n",
    "\n",
    "- **Context / window size**: Quantas janelas de contexto o algoritmo de treinamento deve levar em consideração. 10 parece funcionar bem para o softmax hierarquico (quanto mais melhor, até certo ponto).\n",
    "\n",
    "- **Worker threads**: Numero de processos para correrem em paraleolo. Isso varia de máquina para máquina, mas algo entre 4 e 6 deve funcionar na maioria dos sistemas.\n",
    "\n",
    "- **Minimum word count**: Isso ajuda a limitar o tamanho do vocabulario de palavras significativas. Qualquer palavra que não ocorre ao menos esse número de vezes considerando **todos** os documentos é ignorada. Valores razoáveis são entre 10 e 100. Nesse caso, dado que cada filme ocorre 30 vezes, o valor setado de _minimum word count_ é de 40. Quanto maior o vocabulário, maior o tempo de execução é impactado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-29 02:15:44,723 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2017-11-29 02:15:44,729 : INFO : collecting all words and their counts\n",
      "2017-11-29 02:15:44,730 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-11-29 02:15:44,791 : INFO : PROGRESS: at sentence #10000, processed 227183 words, keeping 18050 word types\n",
      "2017-11-29 02:15:44,857 : INFO : PROGRESS: at sentence #20000, processed 454403 words, keeping 25345 word types\n",
      "2017-11-29 02:15:44,915 : INFO : PROGRESS: at sentence #30000, processed 675010 words, keeping 30508 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-29 02:15:44,986 : INFO : PROGRESS: at sentence #40000, processed 902687 words, keeping 34903 word types\n",
      "2017-11-29 02:15:45,051 : INFO : PROGRESS: at sentence #50000, processed 1123085 words, keeping 38385 word types\n",
      "2017-11-29 02:15:45,119 : INFO : PROGRESS: at sentence #60000, processed 1345767 words, keeping 41405 word types\n",
      "2017-11-29 02:15:45,181 : INFO : PROGRESS: at sentence #70000, processed 1570193 words, keeping 44067 word types\n",
      "2017-11-29 02:15:45,245 : INFO : PROGRESS: at sentence #80000, processed 1790610 words, keeping 46494 word types\n",
      "2017-11-29 02:15:45,312 : INFO : PROGRESS: at sentence #90000, processed 2016019 words, keeping 48972 word types\n",
      "2017-11-29 02:15:45,381 : INFO : PROGRESS: at sentence #100000, processed 2239092 words, keeping 51099 word types\n",
      "2017-11-29 02:15:45,445 : INFO : PROGRESS: at sentence #110000, processed 2460033 words, keeping 53028 word types\n",
      "2017-11-29 02:15:45,512 : INFO : PROGRESS: at sentence #120000, processed 2683309 words, keeping 55117 word types\n",
      "2017-11-29 02:15:45,600 : INFO : PROGRESS: at sentence #130000, processed 2910183 words, keeping 56913 word types\n",
      "2017-11-29 02:15:45,668 : INFO : PROGRESS: at sentence #140000, processed 3124127 words, keeping 58465 word types\n",
      "2017-11-29 02:15:45,735 : INFO : PROGRESS: at sentence #150000, processed 3350995 words, keeping 60237 word types\n",
      "2017-11-29 02:15:45,803 : INFO : PROGRESS: at sentence #160000, processed 3574834 words, keeping 61847 word types\n",
      "2017-11-29 02:15:45,871 : INFO : PROGRESS: at sentence #170000, processed 3799372 words, keeping 63350 word types\n",
      "2017-11-29 02:15:45,935 : INFO : PROGRESS: at sentence #180000, processed 4021142 words, keeping 64814 word types\n",
      "2017-11-29 02:15:46,004 : INFO : PROGRESS: at sentence #190000, processed 4247580 words, keeping 66139 word types\n",
      "2017-11-29 02:15:46,072 : INFO : PROGRESS: at sentence #200000, processed 4472913 words, keeping 67469 word types\n",
      "2017-11-29 02:15:46,137 : INFO : PROGRESS: at sentence #210000, processed 4695594 words, keeping 68811 word types\n",
      "2017-11-29 02:15:46,201 : INFO : PROGRESS: at sentence #220000, processed 4921860 words, keeping 70163 word types\n",
      "2017-11-29 02:15:46,267 : INFO : PROGRESS: at sentence #230000, processed 5145658 words, keeping 71452 word types\n",
      "2017-11-29 02:15:46,335 : INFO : PROGRESS: at sentence #240000, processed 5374409 words, keeping 72695 word types\n",
      "2017-11-29 02:15:46,404 : INFO : PROGRESS: at sentence #250000, processed 5589727 words, keeping 73923 word types\n",
      "2017-11-29 02:15:46,468 : INFO : PROGRESS: at sentence #260000, processed 5810887 words, keeping 75094 word types\n",
      "2017-11-29 02:15:46,529 : INFO : PROGRESS: at sentence #270000, processed 6033450 words, keeping 76440 word types\n",
      "2017-11-29 02:15:46,604 : INFO : PROGRESS: at sentence #280000, processed 6260506 words, keeping 78080 word types\n",
      "2017-11-29 02:15:46,665 : INFO : PROGRESS: at sentence #290000, processed 6484865 words, keeping 79588 word types\n",
      "2017-11-29 02:15:46,738 : INFO : PROGRESS: at sentence #300000, processed 6710734 words, keeping 80958 word types\n",
      "2017-11-29 02:15:46,804 : INFO : PROGRESS: at sentence #310000, processed 6937218 words, keeping 82315 word types\n",
      "2017-11-29 02:15:46,866 : INFO : PROGRESS: at sentence #320000, processed 7163316 words, keeping 83687 word types\n",
      "2017-11-29 02:15:46,930 : INFO : PROGRESS: at sentence #330000, processed 7386325 words, keeping 84974 word types\n",
      "2017-11-29 02:15:46,996 : INFO : PROGRESS: at sentence #340000, processed 7617027 words, keeping 86257 word types\n",
      "2017-11-29 02:15:47,072 : INFO : PROGRESS: at sentence #350000, processed 7841544 words, keeping 87452 word types\n",
      "2017-11-29 02:15:47,131 : INFO : PROGRESS: at sentence #360000, processed 8063329 words, keeping 88653 word types\n",
      "2017-11-29 02:15:47,196 : INFO : PROGRESS: at sentence #370000, processed 8291698 words, keeping 89807 word types\n",
      "2017-11-29 02:15:47,262 : INFO : PROGRESS: at sentence #380000, processed 8518070 words, keeping 91023 word types\n",
      "2017-11-29 02:15:47,326 : INFO : PROGRESS: at sentence #390000, processed 8749088 words, keeping 92101 word types\n",
      "2017-11-29 02:15:47,409 : INFO : PROGRESS: at sentence #400000, processed 8973214 words, keeping 93161 word types\n",
      "2017-11-29 02:15:47,480 : INFO : PROGRESS: at sentence #410000, processed 9195775 words, keeping 94154 word types\n",
      "2017-11-29 02:15:47,553 : INFO : PROGRESS: at sentence #420000, processed 9418110 words, keeping 95222 word types\n",
      "2017-11-29 02:15:47,615 : INFO : PROGRESS: at sentence #430000, processed 9646985 words, keeping 96274 word types\n",
      "2017-11-29 02:15:47,689 : INFO : PROGRESS: at sentence #440000, processed 9875019 words, keeping 97287 word types\n",
      "2017-11-29 02:15:47,775 : INFO : PROGRESS: at sentence #450000, processed 10099927 words, keeping 98452 word types\n",
      "2017-11-29 02:15:47,853 : INFO : PROGRESS: at sentence #460000, processed 10333974 words, keeping 99536 word types\n",
      "2017-11-29 02:15:47,924 : INFO : PROGRESS: at sentence #470000, processed 10563044 words, keeping 100419 word types\n",
      "2017-11-29 02:15:48,001 : INFO : PROGRESS: at sentence #480000, processed 10784580 words, keeping 101382 word types\n",
      "2017-11-29 02:15:48,088 : INFO : PROGRESS: at sentence #490000, processed 11012597 words, keeping 102450 word types\n",
      "2017-11-29 02:15:48,167 : INFO : PROGRESS: at sentence #500000, processed 11235494 words, keeping 103380 word types\n",
      "2017-11-29 02:15:48,240 : INFO : PROGRESS: at sentence #510000, processed 11462005 words, keeping 104342 word types\n",
      "2017-11-29 02:15:48,315 : INFO : PROGRESS: at sentence #520000, processed 11686658 words, keeping 105264 word types\n",
      "2017-11-29 02:15:48,384 : INFO : PROGRESS: at sentence #530000, processed 11912305 words, keeping 106111 word types\n",
      "2017-11-29 02:15:48,466 : INFO : PROGRESS: at sentence #540000, processed 12138211 words, keeping 107009 word types\n",
      "2017-11-29 02:15:48,537 : INFO : PROGRESS: at sentence #550000, processed 12365006 words, keeping 107906 word types\n",
      "2017-11-29 02:15:48,610 : INFO : PROGRESS: at sentence #560000, processed 12587484 words, keeping 108794 word types\n",
      "2017-11-29 02:15:48,690 : INFO : PROGRESS: at sentence #570000, processed 12817861 words, keeping 109608 word types\n",
      "2017-11-29 02:15:48,763 : INFO : PROGRESS: at sentence #580000, processed 13040564 words, keeping 110503 word types\n",
      "2017-11-29 02:15:48,834 : INFO : PROGRESS: at sentence #590000, processed 13267197 words, keeping 111387 word types\n",
      "2017-11-29 02:15:48,908 : INFO : PROGRESS: at sentence #600000, processed 13490648 words, keeping 112149 word types\n",
      "2017-11-29 02:15:48,975 : INFO : PROGRESS: at sentence #610000, processed 13712854 words, keeping 113057 word types\n",
      "2017-11-29 02:15:49,048 : INFO : PROGRESS: at sentence #620000, processed 13940522 words, keeping 113843 word types\n",
      "2017-11-29 02:15:49,113 : INFO : PROGRESS: at sentence #630000, processed 14166025 words, keeping 114653 word types\n",
      "2017-11-29 02:15:49,187 : INFO : PROGRESS: at sentence #640000, processed 14387994 words, keeping 115502 word types\n",
      "2017-11-29 02:15:49,261 : INFO : PROGRESS: at sentence #650000, processed 14615128 words, keeping 116322 word types\n",
      "2017-11-29 02:15:49,338 : INFO : PROGRESS: at sentence #660000, processed 14839125 words, keeping 117111 word types\n",
      "2017-11-29 02:15:49,408 : INFO : PROGRESS: at sentence #670000, processed 15063726 words, keeping 117841 word types\n",
      "2017-11-29 02:15:49,478 : INFO : PROGRESS: at sentence #680000, processed 15289807 words, keeping 118573 word types\n",
      "2017-11-29 02:15:49,551 : INFO : PROGRESS: at sentence #690000, processed 15513127 words, keeping 119384 word types\n",
      "2017-11-29 02:15:49,642 : INFO : PROGRESS: at sentence #700000, processed 15742984 words, keeping 120226 word types\n",
      "2017-11-29 02:15:49,715 : INFO : PROGRESS: at sentence #710000, processed 15967181 words, keeping 120895 word types\n",
      "2017-11-29 02:15:49,780 : INFO : PROGRESS: at sentence #720000, processed 16193688 words, keeping 121537 word types\n",
      "2017-11-29 02:15:49,846 : INFO : PROGRESS: at sentence #730000, processed 16421235 words, keeping 122286 word types\n",
      "2017-11-29 02:15:49,906 : INFO : PROGRESS: at sentence #740000, processed 16643523 words, keeping 123026 word types\n",
      "2017-11-29 02:15:49,973 : INFO : PROGRESS: at sentence #750000, processed 16863120 words, keeping 123677 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-29 02:15:50,035 : INFO : PROGRESS: at sentence #760000, processed 17083737 words, keeping 124327 word types\n",
      "2017-11-29 02:15:50,106 : INFO : PROGRESS: at sentence #770000, processed 17312165 words, keeping 125128 word types\n",
      "2017-11-29 02:15:50,173 : INFO : PROGRESS: at sentence #780000, processed 17543573 words, keeping 125864 word types\n",
      "2017-11-29 02:15:50,242 : INFO : PROGRESS: at sentence #790000, processed 17771837 words, keeping 126562 word types\n",
      "2017-11-29 02:15:50,281 : INFO : collected 127017 word types from a corpus of 17895591 raw words and 795538 sentences\n",
      "2017-11-29 02:15:50,282 : INFO : Loading a fresh vocabulary\n",
      "2017-11-29 02:15:50,377 : INFO : min_count=40 retains 16717 unique words (13% of original 127017, drops 110300)\n",
      "2017-11-29 02:15:50,378 : INFO : min_count=40 leaves 17328137 word corpus (96% of original 17895591, drops 567454)\n",
      "2017-11-29 02:15:50,432 : INFO : deleting the raw counts dictionary of 127017 items\n",
      "2017-11-29 02:15:50,438 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2017-11-29 02:15:50,439 : INFO : downsampling leaves estimated 12855820 word corpus (74.2% of prior 17328137)\n",
      "2017-11-29 02:15:50,440 : INFO : estimated required memory for 16717 words and 300 dimensions: 48479300 bytes\n",
      "2017-11-29 02:15:50,508 : INFO : resetting layer weights\n",
      "2017-11-29 02:15:50,857 : INFO : training model with 4 workers on 16717 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-29 02:15:51,875 : INFO : PROGRESS: at 1.20% examples, 770097 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:15:52,879 : INFO : PROGRESS: at 2.28% examples, 724332 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:15:53,881 : INFO : PROGRESS: at 3.70% examples, 783508 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:15:54,889 : INFO : PROGRESS: at 5.23% examples, 831407 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:15:55,896 : INFO : PROGRESS: at 6.47% examples, 821561 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:15:56,896 : INFO : PROGRESS: at 8.13% examples, 862553 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:15:57,901 : INFO : PROGRESS: at 9.79% examples, 892193 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:15:58,908 : INFO : PROGRESS: at 11.24% examples, 896432 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:15:59,912 : INFO : PROGRESS: at 12.97% examples, 921400 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:00,914 : INFO : PROGRESS: at 14.69% examples, 939432 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:01,929 : INFO : PROGRESS: at 16.46% examples, 956275 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:02,931 : INFO : PROGRESS: at 17.80% examples, 948251 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:03,933 : INFO : PROGRESS: at 19.21% examples, 944709 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:04,938 : INFO : PROGRESS: at 20.73% examples, 947045 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:05,950 : INFO : PROGRESS: at 21.93% examples, 934337 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:06,954 : INFO : PROGRESS: at 23.08% examples, 921907 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:07,967 : INFO : PROGRESS: at 24.36% examples, 914671 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:08,972 : INFO : PROGRESS: at 25.78% examples, 914172 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-29 02:16:09,976 : INFO : PROGRESS: at 27.12% examples, 910747 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:10,983 : INFO : PROGRESS: at 28.44% examples, 907563 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:11,985 : INFO : PROGRESS: at 29.80% examples, 906257 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:12,993 : INFO : PROGRESS: at 31.19% examples, 905489 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:13,995 : INFO : PROGRESS: at 32.54% examples, 904052 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:15,000 : INFO : PROGRESS: at 33.96% examples, 904471 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:16,006 : INFO : PROGRESS: at 35.21% examples, 900224 words/s, in_qsize 7, out_qsize 3\n",
      "2017-11-29 02:16:17,010 : INFO : PROGRESS: at 36.11% examples, 887596 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:18,010 : INFO : PROGRESS: at 37.50% examples, 887894 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:19,010 : INFO : PROGRESS: at 38.88% examples, 887941 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:20,013 : INFO : PROGRESS: at 40.30% examples, 888862 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:21,019 : INFO : PROGRESS: at 41.86% examples, 892262 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:22,024 : INFO : PROGRESS: at 43.32% examples, 893218 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:23,028 : INFO : PROGRESS: at 44.91% examples, 896946 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:24,035 : INFO : PROGRESS: at 46.57% examples, 901674 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:25,037 : INFO : PROGRESS: at 48.15% examples, 905060 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:26,041 : INFO : PROGRESS: at 49.34% examples, 901022 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:27,044 : INFO : PROGRESS: at 50.85% examples, 903011 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:28,046 : INFO : PROGRESS: at 52.22% examples, 902578 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:29,047 : INFO : PROGRESS: at 53.87% examples, 906916 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:30,048 : INFO : PROGRESS: at 55.43% examples, 909389 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:31,049 : INFO : PROGRESS: at 57.01% examples, 911891 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:32,051 : INFO : PROGRESS: at 58.59% examples, 914438 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:33,053 : INFO : PROGRESS: at 60.16% examples, 916662 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:34,057 : INFO : PROGRESS: at 61.80% examples, 919766 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:35,061 : INFO : PROGRESS: at 63.29% examples, 920316 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:36,067 : INFO : PROGRESS: at 64.75% examples, 920441 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:37,070 : INFO : PROGRESS: at 66.23% examples, 920940 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:38,075 : INFO : PROGRESS: at 67.57% examples, 919546 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:39,077 : INFO : PROGRESS: at 69.05% examples, 920217 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:40,082 : INFO : PROGRESS: at 70.63% examples, 922113 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:41,083 : INFO : PROGRESS: at 72.01% examples, 921719 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:42,084 : INFO : PROGRESS: at 73.62% examples, 923865 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:43,088 : INFO : PROGRESS: at 75.23% examples, 925870 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:44,096 : INFO : PROGRESS: at 76.84% examples, 927886 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:45,099 : INFO : PROGRESS: at 78.49% examples, 930293 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:46,102 : INFO : PROGRESS: at 80.18% examples, 933108 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:47,105 : INFO : PROGRESS: at 81.89% examples, 935970 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:48,110 : INFO : PROGRESS: at 83.55% examples, 937968 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:49,112 : INFO : PROGRESS: at 85.23% examples, 940293 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:50,113 : INFO : PROGRESS: at 86.91% examples, 942546 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:51,114 : INFO : PROGRESS: at 88.56% examples, 944493 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:52,121 : INFO : PROGRESS: at 90.21% examples, 946419 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:53,128 : INFO : PROGRESS: at 91.80% examples, 947677 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:54,131 : INFO : PROGRESS: at 93.45% examples, 949415 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:55,134 : INFO : PROGRESS: at 95.16% examples, 951680 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:56,138 : INFO : PROGRESS: at 96.79% examples, 953082 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:57,147 : INFO : PROGRESS: at 98.37% examples, 954040 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-29 02:16:58,152 : INFO : PROGRESS: at 99.79% examples, 953334 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-29 02:16:58,290 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-11-29 02:16:58,296 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-29 02:16:58,301 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-29 02:16:58,307 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-29 02:16:58,307 : INFO : training on 89477955 raw words (64280920 effective words) took 67.4s, 953145 effective words/s\n",
      "2017-11-29 02:16:58,309 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-11-29 02:16:58,478 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2017-11-29 02:16:58,479 : INFO : not storing attribute syn0norm\n",
      "2017-11-29 02:16:58,480 : INFO : not storing attribute cum_table\n",
      "2017-11-29 02:16:59,127 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features =     # Word vector dimensionality                      \n",
    "min_word_count =    # Minimum word count                        \n",
    "num_workers =        # Number of threads to run in parallel\n",
    "context =           # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print (\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=, \\\n",
    "            size=, min_count = , \\\n",
    "            window = , sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorando os resultados\n",
    "\n",
    "A função `doesnt_match` tentará deduzir qual palavra em um set é a mais _distinta_ das outras.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apenas paises\n",
    "model.doesnt_match(\"\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma análoga, a função `most_similar` tentará deduzir qual palavra em um set é a mais _semelhante_ das outras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7730972766876221),\n",
       " ('horrible', 0.7526340484619141),\n",
       " ('atrocious', 0.7306756973266602),\n",
       " ('dreadful', 0.7170031666755676),\n",
       " ('abysmal', 0.7054095268249512),\n",
       " ('horrendous', 0.6861761808395386),\n",
       " ('horrid', 0.6846473217010498),\n",
       " ('appalling', 0.6617923974990845),\n",
       " ('lousy', 0.6321056485176086),\n",
       " ('crappy', 0.615153431892395)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"\")\n",
    "\n",
    "#podemos ate ter um indicio para sentiment analysis\n",
    "model.most_similar(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Mas como definir similaridade? Que uma palavra é similar à outra?\n",
    "\n",
    "A métrica mais famosa para isso é a _Similaridade de Coscenos_ ou _Cosine Similarity_.\n",
    "\n",
    "Vamos voltar à definição de espaço semantico dada no começo da aula.\n",
    "\n",
    "![Male-Female](img/male-female.png)\n",
    "\n",
    "Olhando um pouco debaixo dos panos (porque na situação real é como se estivessemos olhando para um vetor de 300 dimensões, o que é muito abstrato), vamos projetar **apenas** essa situação de _Male-Female_\n",
    "\n",
    "\n",
    "![Male-Female](img/sematic-male-female.png)\n",
    "\n",
    "\n",
    "Ao olharmos esse espaço, vemos claramente que as palavras semânticamente próximas a _male_ também são próximas entre si. O mesmo acontece com _female_. Mas como que a gente encontra esse *valor*? Basta calcularmos o **cosceno** entre os vetores que formam essas palavras !\n",
    "\n",
    "Lembrando um pouquinho de matemática da faculdade, dado o produto escalar entre dois vetores e suas normas eu consigo encontrar o cosceno formado entre os dois:\n",
    "\n",
    "$A \\cdot B = \\lVert A \\rVert \\lVert B \\rVert \\cos\\theta   $\n",
    "\n",
    "![Cosine](img/cosine-similarity.png)\n",
    "\n",
    "\n",
    "Em NLP essa métrica é muito comum quando queremos calcular a similaridade entre documentos.\n",
    "\n",
    "## Entendendo a representação numérica das palavras\n",
    "\n",
    "Agora nós treinamos um modelo que consegue entender a relação semântica entre as palavras. Para algumas aplicações é exatamente isso que a gente quer, mas o que podemos fazer com isso? Os vetores de palavras criados pelo nosso modelo foram armazenados em um array `numpy` chamado `syn0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-29 02:16:59,201 : INFO : loading Word2Vec object from 300features_40minwords_10context\n",
      "2017-11-29 02:16:59,553 : INFO : loading wv recursively from 300features_40minwords_10context.wv.* with mmap=None\n",
      "2017-11-29 02:16:59,554 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-11-29 02:16:59,555 : INFO : setting ignored attribute cum_table to None\n",
      "2017-11-29 02:16:59,556 : INFO : loaded 300features_40minwords_10context\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec \n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")\n",
    "type(model.wv.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16717, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembre-se que a gente limitou o número de ocorrências de palavras para um mínimo de 40. Isso justifica a menor quantidade de vocabulário, sendo formado por 16717 palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.14542146e-02,   1.36064023e-01,  -4.50171567e-02,\n",
       "         3.66699137e-02,   5.50786778e-03,  -4.65489067e-02,\n",
       "         9.40594971e-02,   3.61240245e-02,  -1.84720531e-02,\n",
       "        -4.95337918e-02,   4.64605819e-03,  -8.72984678e-02,\n",
       "        -3.61285551e-04,   9.04739499e-02,  -5.85301267e-03,\n",
       "         9.78970993e-03,  -8.90556071e-03,   6.74419105e-02,\n",
       "         2.54531633e-02,  -1.24042602e-02,  -1.13968197e-02,\n",
       "         1.45032071e-02,  -5.83921298e-02,  -9.96814389e-03,\n",
       "         1.35538206e-01,   8.33753794e-02,  -6.16430538e-03,\n",
       "         3.69862095e-02,   8.45942795e-02,   1.28843904e-01,\n",
       "         3.15543227e-02,  -1.10337615e-01,  -6.18141070e-02,\n",
       "        -9.54130739e-02,   9.34280083e-02,   4.09199670e-02,\n",
       "        -4.39016633e-02,   2.57810298e-02,  -2.82172319e-02,\n",
       "         6.09959327e-02,   1.52271669e-02,   3.31028625e-02,\n",
       "        -9.84279960e-02,   5.35987169e-02,   9.68147144e-02,\n",
       "         4.40609679e-02,  -4.52714823e-02,   6.65505044e-03,\n",
       "         1.26678981e-02,  -1.09604709e-01,  -8.38358849e-02,\n",
       "        -4.07436304e-02,   1.15078669e-02,   5.50224818e-02,\n",
       "        -6.75990954e-02,  -2.10693888e-02,  -6.50745491e-03,\n",
       "         2.92295720e-02,  -1.64019112e-02,  -5.31532802e-02,\n",
       "        -1.22789197e-01,   1.29516527e-01,  -4.80960868e-02,\n",
       "         9.04992372e-02,  -6.60435781e-02,   1.39278537e-02,\n",
       "         3.84474471e-02,  -6.07657693e-02,   2.32478082e-02,\n",
       "        -1.04084067e-01,  -6.45755529e-02,   5.52406982e-02,\n",
       "        -9.76696834e-02,  -1.03100218e-01,  -4.66173217e-02,\n",
       "         1.00660495e-01,   2.42150631e-02,  -7.58552775e-02,\n",
       "         4.25034724e-02,   5.81701286e-02,  -3.27274799e-02,\n",
       "         7.01337308e-02,   1.48298889e-01,  -8.92738253e-02,\n",
       "         8.68753791e-02,  -2.09502280e-02,   6.08585961e-03,\n",
       "        -1.73648987e-02,   4.24945727e-02,  -1.16290815e-01,\n",
       "        -7.31675550e-02,   7.88055062e-02,   1.19979560e-01,\n",
       "        -2.56362162e-03,   5.25949597e-02,   4.11265381e-02,\n",
       "        -3.50094065e-02,   2.20412649e-02,  -1.20696500e-02,\n",
       "        -1.11523472e-01,   3.27084847e-02,  -6.65100105e-03,\n",
       "        -1.63885783e-02,   4.08368595e-02,  -7.25353584e-02,\n",
       "         8.50190744e-02,  -1.88424829e-02,   2.05983464e-02,\n",
       "        -1.07706515e-02,   1.25898883e-01,  -9.04607121e-03,\n",
       "         6.98906854e-02,   1.22875348e-01,  -6.92799361e-03,\n",
       "         3.93490903e-02,   3.49719450e-02,  -3.88762914e-02,\n",
       "         9.07566324e-02,   5.87833710e-02,   4.31419238e-02,\n",
       "         2.49127317e-02,   8.83819982e-02,  -6.03614375e-02,\n",
       "         8.24141782e-03,   5.11494316e-02,  -9.80071723e-02,\n",
       "         1.93612296e-02,  -5.29685654e-02,  -5.44945002e-02,\n",
       "        -1.36991978e-01,   6.49298429e-02,  -2.81639416e-02,\n",
       "         3.07198372e-02,  -8.64285883e-03,   4.34266403e-02,\n",
       "         1.83144379e-02,   3.92479338e-02,  -5.97372964e-05,\n",
       "        -3.94414999e-02,   1.15318708e-01,   1.22377863e-02,\n",
       "         3.34539860e-02,  -2.23781578e-02,   3.82340439e-02,\n",
       "        -8.61035213e-02,   1.27679497e-01,   3.60193104e-02,\n",
       "        -2.88205147e-02,  -7.06016971e-03,  -1.06701463e-01,\n",
       "         8.87490213e-02,  -5.19642495e-02,  -9.15159211e-02,\n",
       "         7.33661233e-03,  -8.80158506e-03,   5.80244511e-02,\n",
       "        -4.80028451e-04,   5.71291242e-03,  -4.66242656e-02,\n",
       "         4.69504558e-02,   4.81312722e-02,  -1.93940066e-02,\n",
       "        -2.22978443e-02,   7.49439821e-02,   2.83605829e-02,\n",
       "        -3.42507288e-02,   5.67686222e-02,   4.12511118e-02,\n",
       "        -3.48903500e-02,   6.26885667e-02,  -6.21465314e-03,\n",
       "        -4.41388860e-02,  -2.18880400e-02,  -1.80419534e-02,\n",
       "         2.25774795e-02,  -6.25847140e-03,   2.84490511e-02,\n",
       "         5.15723089e-03,   6.90031499e-02,  -7.40054622e-02,\n",
       "         4.59456518e-02,   4.42532152e-02,  -8.24082922e-03,\n",
       "         1.80489961e-02,  -3.76109295e-02,  -2.81857457e-02,\n",
       "         9.83137786e-02,   6.00439459e-02,   6.05824366e-02,\n",
       "        -4.59882338e-03,  -2.03148518e-02,  -3.55501100e-02,\n",
       "         4.03514765e-02,   2.90138535e-02,   6.14448870e-03,\n",
       "        -9.52479336e-03,  -1.00576393e-02,   5.12761027e-02,\n",
       "        -5.33474702e-03,   4.31212261e-02,  -1.83657091e-02,\n",
       "        -3.63900028e-02,   1.22378953e-02,   3.86323109e-02,\n",
       "        -4.04827781e-02,  -8.03851485e-02,  -5.26394770e-02,\n",
       "         2.58516744e-02,   1.58291742e-01,  -3.52208167e-02,\n",
       "         1.48541024e-02,  -2.46044379e-02,   6.76241145e-02,\n",
       "         1.99764669e-02,   2.67461520e-02,   2.15291753e-02,\n",
       "         5.61976545e-02,  -3.52509916e-02,   1.01541191e-01,\n",
       "        -4.04607169e-02,  -5.31902686e-02,  -3.82821821e-02,\n",
       "        -1.10847000e-02,   1.01943254e-01,  -7.76224285e-02,\n",
       "         5.55046797e-02,  -6.22208826e-02,  -1.28488149e-02,\n",
       "        -1.27895707e-02,  -9.76919383e-02,   7.28219468e-03,\n",
       "        -1.33950775e-03,   5.67820901e-03,  -4.06499058e-02,\n",
       "        -2.85615511e-02,  -5.99113144e-02,  -5.51051348e-02,\n",
       "         2.69500120e-03,   7.88963493e-03,  -5.49384281e-02,\n",
       "        -4.78386022e-02,   1.54650901e-02,   1.05688624e-01,\n",
       "         5.55348210e-02,  -1.19066390e-03,   4.15842831e-02,\n",
       "        -7.00480193e-02,   5.93183469e-03,  -2.57273559e-02,\n",
       "        -3.21034603e-02,   4.08638082e-02,   8.31957683e-02,\n",
       "         1.89259015e-02,  -1.88706387e-02,  -6.89588785e-02,\n",
       "        -3.49206454e-03,  -6.48809373e-02,  -3.14461291e-02,\n",
       "         1.79878669e-03,   4.91885468e-02,   9.90622714e-02,\n",
       "         9.94907841e-02,  -1.18799262e-01,   4.13593017e-02,\n",
       "        -2.37568375e-02,  -2.35974211e-02,  -2.07283385e-02,\n",
       "        -3.70032503e-03,   5.17149456e-02,  -1.09722309e-01,\n",
       "         3.30641791e-02,  -2.58193780e-02,  -9.23065003e-03,\n",
       "        -4.74296361e-02,   4.36393358e-02,   8.72100238e-03,\n",
       "        -3.70101929e-02,   9.07310471e-02,   1.17740389e-02,\n",
       "         8.70065065e-04,   3.61709818e-02,  -1.00157261e-01,\n",
       "        -5.54043911e-02,  -3.75497490e-02,   5.31354686e-03,\n",
       "         8.41007531e-02,  -9.14202332e-02,  -1.30246216e-02,\n",
       "         2.73502078e-02,   3.41082923e-02,  -5.87837910e-03,\n",
       "        -7.95173794e-02,  -5.09417839e-02,   4.25858870e-02,\n",
       "        -1.29130566e-02,   4.47501056e-03,   9.26870108e-02,\n",
       "        -1.92635879e-02,   4.93311556e-03,  -3.31702344e-02], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"flower\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model[\"flower\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De palavras para reviews \n",
    "\n",
    "Um problema que temos ao lidar com o dataset do IMDB é que os reviews tem tamanho variado. Precisaríamos, então, encontrar uma forma de pegar esses vetores de palavras e transformá-los em um conjunto de features que tem o mesmo tamanho para o mesmo _review_.\n",
    "\n",
    "Já que todas as palavras são representadas por um vetor de 300 dimensões, nós podemos usar operações vetoriais para combinar palavras. Por exemplo, podemos pegar todas as palavras que compõe um review e somá-las ou tirar a média. No caso, vamos tirar a média (contudo, aqui nós não precisamos mais do contexto das palavras e, então, podemos remover stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np  # Make sure that numpy is imported\n",
    "\n",
    "def makeFeatureVec(review, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    #Hint: np.add should be useful\n",
    "    \n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    #Hint: use np.divide\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in tqdm(reviews,total=len(reviews)):\n",
    "\n",
    "    #  Implement here the function that will calculate the average for each review     \n",
    "    #\n",
    "\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:58<00:00, 425.65it/s]\n",
      "100%|██████████| 25000/25000 [00:34<00:00, 726.27it/s]\n",
      "  0%|          | 56/25000 [00:00<00:45, 548.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for test reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:54<00:00, 460.40it/s]\n",
      "100%|██████████| 25000/25000 [00:33<00:00, 746.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "\n",
    "clean_train_reviews = []\n",
    "\n",
    "\n",
    "trainDataVecs = \n",
    "\n",
    "print (\"Creating average feature vecs for test reviews\")\n",
    "clean_test_reviews = []\n",
    "\n",
    "\n",
    "testDataVecs = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui, como a ideia é usar o vetor de features como treino, estamos lidando novamente com um aprendizado **surpervisionado** e, então, usaremos apenas os dados com _label_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>\"Afraid of the Dark left me with the impressio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>\"A very accurate depiction of small time mob l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"2913_8\"</td>\n",
       "      <td>\"...as valuable as King Tut's tomb! (OK, maybe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"4396_1\"</td>\n",
       "      <td>\"This has to be one of the biggest misfires ev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"395_2\"</td>\n",
       "      <td>\"This is one of those movies I watched, and wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"10616_1\"</td>\n",
       "      <td>\"The worst movie i've seen in years (and i've ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"9074_9\"</td>\n",
       "      <td>\"Five medical students (Kevin Bacon, David Lab...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"9252_3\"</td>\n",
       "      <td>\"'The Mill on the Floss' was one of the lesser...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"9896_9\"</td>\n",
       "      <td>\"I just saw this film at the phoenix film fest...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\"574_4\"</td>\n",
       "      <td>\"\\\"The Love Letter\\\" is one of those movies th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"11182_8\"</td>\n",
       "      <td>\"Another fantastic offering from the Monkey Is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\"11656_4\"</td>\n",
       "      <td>\"This was included on the disk \\\"Shorts: Volum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"2322_4\"</td>\n",
       "      <td>\"I'm not really much of an Abbott &amp; Costello f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\"8703_1\"</td>\n",
       "      <td>\"This movie was dreadful. Biblically very inac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\"7483_1\"</td>\n",
       "      <td>\"I don't think I've ever gave something a 1/10...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\"6007_10\"</td>\n",
       "      <td>\"Excellent story-telling and cinematography. P...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\"12424_4\"</td>\n",
       "      <td>\"I completely forgot that I'd seen this within...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>\"4672_1\"</td>\n",
       "      <td>\"I like action movies. I have a softspot for \\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>\"10841_3\"</td>\n",
       "      <td>\"This is one of the worst Sandra Bullock movie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\"8954_7\"</td>\n",
       "      <td>\"Watched this flick on Saturday afternoon cabl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>\"7392_1\"</td>\n",
       "      <td>\"I went to see \\\"TKIA\\\" with high expectations...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>\"10288_8\"</td>\n",
       "      <td>\"All credit to writer/director Gilles Mimouni ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>\"5343_4\"</td>\n",
       "      <td>\"As a writing teacher, there are two ending I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>\"4950_1\"</td>\n",
       "      <td>\"I don't know why this has gotten any decent r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>\"9257_4\"</td>\n",
       "      <td>\"This film was released in the UK under the na...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>\"8689_3\"</td>\n",
       "      <td>\"Uncle Fred Olen Ray once again gives us a lit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>\"4480_2\"</td>\n",
       "      <td>\"OK, it's watchable if you are sick in bed or ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24970</th>\n",
       "      <td>\"6857_10\"</td>\n",
       "      <td>\"With \\\"Anatomy\\\" the german film producers ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24971</th>\n",
       "      <td>\"11091_8\"</td>\n",
       "      <td>\"This movie is one of my all-time favorites. I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24972</th>\n",
       "      <td>\"4167_2\"</td>\n",
       "      <td>\"I found Code 46 very disappointing. I thought...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24973</th>\n",
       "      <td>\"679_4\"</td>\n",
       "      <td>\"Tamara Anderson and her family are moving onc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24974</th>\n",
       "      <td>\"10147_1\"</td>\n",
       "      <td>\"Now I've seen it all. Just when I thought it ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24975</th>\n",
       "      <td>\"6875_1\"</td>\n",
       "      <td>\"In this movie everything possible was wrong a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24976</th>\n",
       "      <td>\"923_10\"</td>\n",
       "      <td>\"Well every scene so perfectly presented. Neve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24977</th>\n",
       "      <td>\"6200_8\"</td>\n",
       "      <td>\"Sleeper Cell is what 24 should have been. 24 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24978</th>\n",
       "      <td>\"7208_8\"</td>\n",
       "      <td>\"Not for everyone, but I really like it. Nice ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24979</th>\n",
       "      <td>\"5363_8\"</td>\n",
       "      <td>\"Set just before the Second World War, this is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24980</th>\n",
       "      <td>\"4067_8\"</td>\n",
       "      <td>\"Contains Spoiler The movie is a good action/c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24981</th>\n",
       "      <td>\"1773_7\"</td>\n",
       "      <td>\"This is one of several period sea-faring yarn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24982</th>\n",
       "      <td>\"1498_10\"</td>\n",
       "      <td>\"Hearkening back to those \\\"Good Old Days\\\" of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24983</th>\n",
       "      <td>\"10497_10\"</td>\n",
       "      <td>\"I thought this to be a pretty good example of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24984</th>\n",
       "      <td>\"3444_10\"</td>\n",
       "      <td>\"Seeing this film, or rather set of films, in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24985</th>\n",
       "      <td>\"588_2\"</td>\n",
       "      <td>\"I didn't like this movie for many reasons - V...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24986</th>\n",
       "      <td>\"9678_9\"</td>\n",
       "      <td>\"I absolutely love this show!!!!!!!, Its basic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24987</th>\n",
       "      <td>\"1983_9\"</td>\n",
       "      <td>\"eXistenZ combines director David Cronenberg's...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24988</th>\n",
       "      <td>\"5012_3\"</td>\n",
       "      <td>\"this movie is allegedly a comedy.so where did...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24989</th>\n",
       "      <td>\"12240_2\"</td>\n",
       "      <td>\"The Comebacks is a spoof on inspirational spo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24990</th>\n",
       "      <td>\"5071_2\"</td>\n",
       "      <td>\"I'd love to write a little summary of this mo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24991</th>\n",
       "      <td>\"5078_2\"</td>\n",
       "      <td>\"Obvious tailored vehicle for Ryan Philippe. I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24992</th>\n",
       "      <td>\"10069_3\"</td>\n",
       "      <td>\"&lt;br /&gt;&lt;br /&gt;JURASSIC PARK III *___ Adventure ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24993</th>\n",
       "      <td>\"7407_8\"</td>\n",
       "      <td>\"If you're even mildly interested in the War b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24994</th>\n",
       "      <td>\"7207_1\"</td>\n",
       "      <td>\"It used to be that video distributors like Su...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>\"2155_10\"</td>\n",
       "      <td>\"Sony Pictures Classics, I'm looking at you! S...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>\"59_10\"</td>\n",
       "      <td>\"I always felt that Ms. Merkerson had never go...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>\"2531_1\"</td>\n",
       "      <td>\"I was so disappointed in this movie. I am ver...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>\"7772_8\"</td>\n",
       "      <td>\"From the opening sequence, filled with black ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>\"11465_10\"</td>\n",
       "      <td>\"This is a great horror film for people who do...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                             review  \\\n",
       "0      \"12311_10\"  \"Naturally in a film who's main themes are of ...   \n",
       "1        \"8348_2\"  \"This movie is a disaster within a disaster fi...   \n",
       "2        \"5828_4\"  \"All in all, this is a movie for kids. We saw ...   \n",
       "3        \"7186_2\"  \"Afraid of the Dark left me with the impressio...   \n",
       "4       \"12128_7\"  \"A very accurate depiction of small time mob l...   \n",
       "5        \"2913_8\"  \"...as valuable as King Tut's tomb! (OK, maybe...   \n",
       "6        \"4396_1\"  \"This has to be one of the biggest misfires ev...   \n",
       "7         \"395_2\"  \"This is one of those movies I watched, and wo...   \n",
       "8       \"10616_1\"  \"The worst movie i've seen in years (and i've ...   \n",
       "9        \"9074_9\"  \"Five medical students (Kevin Bacon, David Lab...   \n",
       "10       \"9252_3\"  \"'The Mill on the Floss' was one of the lesser...   \n",
       "11       \"9896_9\"  \"I just saw this film at the phoenix film fest...   \n",
       "12        \"574_4\"  \"\\\"The Love Letter\\\" is one of those movies th...   \n",
       "13      \"11182_8\"  \"Another fantastic offering from the Monkey Is...   \n",
       "14      \"11656_4\"  \"This was included on the disk \\\"Shorts: Volum...   \n",
       "15       \"2322_4\"  \"I'm not really much of an Abbott & Costello f...   \n",
       "16       \"8703_1\"  \"This movie was dreadful. Biblically very inac...   \n",
       "17       \"7483_1\"  \"I don't think I've ever gave something a 1/10...   \n",
       "18      \"6007_10\"  \"Excellent story-telling and cinematography. P...   \n",
       "19      \"12424_4\"  \"I completely forgot that I'd seen this within...   \n",
       "20       \"4672_1\"  \"I like action movies. I have a softspot for \\...   \n",
       "21      \"10841_3\"  \"This is one of the worst Sandra Bullock movie...   \n",
       "22       \"8954_7\"  \"Watched this flick on Saturday afternoon cabl...   \n",
       "23       \"7392_1\"  \"I went to see \\\"TKIA\\\" with high expectations...   \n",
       "24      \"10288_8\"  \"All credit to writer/director Gilles Mimouni ...   \n",
       "25       \"5343_4\"  \"As a writing teacher, there are two ending I ...   \n",
       "26       \"4950_1\"  \"I don't know why this has gotten any decent r...   \n",
       "27       \"9257_4\"  \"This film was released in the UK under the na...   \n",
       "28       \"8689_3\"  \"Uncle Fred Olen Ray once again gives us a lit...   \n",
       "29       \"4480_2\"  \"OK, it's watchable if you are sick in bed or ...   \n",
       "...           ...                                                ...   \n",
       "24970   \"6857_10\"  \"With \\\"Anatomy\\\" the german film producers ha...   \n",
       "24971   \"11091_8\"  \"This movie is one of my all-time favorites. I...   \n",
       "24972    \"4167_2\"  \"I found Code 46 very disappointing. I thought...   \n",
       "24973     \"679_4\"  \"Tamara Anderson and her family are moving onc...   \n",
       "24974   \"10147_1\"  \"Now I've seen it all. Just when I thought it ...   \n",
       "24975    \"6875_1\"  \"In this movie everything possible was wrong a...   \n",
       "24976    \"923_10\"  \"Well every scene so perfectly presented. Neve...   \n",
       "24977    \"6200_8\"  \"Sleeper Cell is what 24 should have been. 24 ...   \n",
       "24978    \"7208_8\"  \"Not for everyone, but I really like it. Nice ...   \n",
       "24979    \"5363_8\"  \"Set just before the Second World War, this is...   \n",
       "24980    \"4067_8\"  \"Contains Spoiler The movie is a good action/c...   \n",
       "24981    \"1773_7\"  \"This is one of several period sea-faring yarn...   \n",
       "24982   \"1498_10\"  \"Hearkening back to those \\\"Good Old Days\\\" of...   \n",
       "24983  \"10497_10\"  \"I thought this to be a pretty good example of...   \n",
       "24984   \"3444_10\"  \"Seeing this film, or rather set of films, in ...   \n",
       "24985     \"588_2\"  \"I didn't like this movie for many reasons - V...   \n",
       "24986    \"9678_9\"  \"I absolutely love this show!!!!!!!, Its basic...   \n",
       "24987    \"1983_9\"  \"eXistenZ combines director David Cronenberg's...   \n",
       "24988    \"5012_3\"  \"this movie is allegedly a comedy.so where did...   \n",
       "24989   \"12240_2\"  \"The Comebacks is a spoof on inspirational spo...   \n",
       "24990    \"5071_2\"  \"I'd love to write a little summary of this mo...   \n",
       "24991    \"5078_2\"  \"Obvious tailored vehicle for Ryan Philippe. I...   \n",
       "24992   \"10069_3\"  \"<br /><br />JURASSIC PARK III *___ Adventure ...   \n",
       "24993    \"7407_8\"  \"If you're even mildly interested in the War b...   \n",
       "24994    \"7207_1\"  \"It used to be that video distributors like Su...   \n",
       "24995   \"2155_10\"  \"Sony Pictures Classics, I'm looking at you! S...   \n",
       "24996     \"59_10\"  \"I always felt that Ms. Merkerson had never go...   \n",
       "24997    \"2531_1\"  \"I was so disappointed in this movie. I am ver...   \n",
       "24998    \"7772_8\"  \"From the opening sequence, filled with black ...   \n",
       "24999  \"11465_10\"  \"This is a great horror film for people who do...   \n",
       "\n",
       "       sentiment  \n",
       "0              1  \n",
       "1              0  \n",
       "2              1  \n",
       "3              0  \n",
       "4              1  \n",
       "5              1  \n",
       "6              0  \n",
       "7              0  \n",
       "8              0  \n",
       "9              1  \n",
       "10             1  \n",
       "11             1  \n",
       "12             1  \n",
       "13             1  \n",
       "14             0  \n",
       "15             1  \n",
       "16             1  \n",
       "17             0  \n",
       "18             1  \n",
       "19             0  \n",
       "20             0  \n",
       "21             0  \n",
       "22             0  \n",
       "23             1  \n",
       "24             1  \n",
       "25             0  \n",
       "26             0  \n",
       "27             0  \n",
       "28             0  \n",
       "29             0  \n",
       "...          ...  \n",
       "24970          1  \n",
       "24971          1  \n",
       "24972          0  \n",
       "24973          0  \n",
       "24974          0  \n",
       "24975          0  \n",
       "24976          1  \n",
       "24977          0  \n",
       "24978          1  \n",
       "24979          1  \n",
       "24980          0  \n",
       "24981          1  \n",
       "24982          1  \n",
       "24983          1  \n",
       "24984          1  \n",
       "24985          0  \n",
       "24986          1  \n",
       "24987          0  \n",
       "24988          0  \n",
       "24989          1  \n",
       "24990          0  \n",
       "24991          0  \n",
       "24992          0  \n",
       "24993          1  \n",
       "24994          0  \n",
       "24995          1  \n",
       "24996          1  \n",
       "24997          1  \n",
       "24998          1  \n",
       "24999          0  \n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a random forest to the training data, using 100 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n",
    "\n",
    "forest = forest.fit( trainDataVecs, train[\"sentiment\"] )\n",
    "\n",
    "# Test & extract results \n",
    "result = forest.predict( testDataVecs )\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"],\"review\":test[\"review\"],\"sentiment\":result} )\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mas analisando o Word2Vec \n",
    "\n",
    "Os resultados foram melhores que o BoW?\n",
    "\n",
    "Se analisarmos de uma forma minuciosa, vemos que os resultados do BoW foram **um pouco melhores** que o do Word2Vec, mas porquê isso ocorre?\n",
    "\n",
    "O fato da gente utilizar a média para criar as sentenças faz com que nós percamos a ordem das palavras (além de perdermos informação ao resumir os dados por meio da média), fazendo com que a abordagem seja muito parecida com o conceito do Bag Of Words, de forma que os métodos até sejam considerados equivalentes.\n",
    "\n",
    "Algumas coisas para melhorar:\n",
    "- Dados, dados dados. Lembre-se que o paper original usou um corpus de mais de um **bilhão** de palavras enquanto os nossos dados tinham aproximadamente 18 **milhões** de palavras. É possível usar vetorer pré treinados para usar em nossos dados e talvez isso resulte em resultados mais interessantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (somostera)",
   "language": "python",
   "name": "somostera"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
