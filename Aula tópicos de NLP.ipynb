{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos usar o poder de Deep Learning para realizar tarefas de NLP? Bom, podemos pensar em aplicações mais complexas como _Speech Recognition_; _Chatbots_ que entendem perguntas complexas, entre outras situações. Contudo, podemos tirar proveito de NLP com Deep Learning de uma forma \"relativamente\" (áspas intencionais) simples: que é através do _Word2Vec_.\n",
    "\n",
    "O [_Word2Vec_](https://code.google.com/archive/p/word2vec/) é uma aplicação desenvolvida pela Google por volta de 2013 que nada mais é do que um modelo de _Embedding de palavras_. Para aqueles que queiram uma definição mais formal, essa [apresentação](https://docs.google.com/file/d/0B7XkCwpI5KDYRWRnd1RzWXQ2TWc/edit) pode ser util.\n",
    "\n",
    "## Embeddings de Palavras\n",
    "\n",
    "Quando lidamos com palavras em texto, podemos ter uma situação em que temos milhares de classes para prever, uma para cada palavra. Uma abordagem que podemos usar é a conhecida como **one hot encoding**, em que o número de neurônios da nossa Rede Neural equivale ao número de palavras do nosso _dataset_ e ao entrar diferentes palavras, sua posição no vocabulário é setada como 1 e nos demais, 0. Contudo, esse tipo de situação é extreamente ineficiente, já que por exemplo, podemos ter uma situação com apenas um elemento setado como 1 e 50,000 como 0. Computacionalmente é um processo caro.\n",
    "\n",
    "![One Hot Encoding](img/one-hot.png)\n",
    "\n",
    "Para resolver esse problema nós usamos o que pode ser chamado de _Word Embedding_._Embeddings_ nada mais são do que uma camada oculta (a camada de _Embedding_) de uma Rede Neural Artificial como essa que a gente acabou de ver. Nos _Embeddings_, não é preciso computar todos os neurônios, mas apenas pegar a linha dos vetores de peso (_embedding weights_). A gente pode fazer isso porquê a multiplicação de um vetor _one hot encoding_ por essa camada irá resultar apenas no valor da linha correspondente ao 1.\n",
    "\n",
    "![One Hot Multiply](img/one-multiply.png)\n",
    "\n",
    "\n",
    "Logo, quando tratamos com _Embeddings_, computacionalmente falando não ocorre multiplicação matricial, mas apenas um _lookup_. Lembra que os computadores entendem apenas números? Então, suponha que a palavra _house_ tenha sido encodada na linha 958 enquanto _life_ na 1056. Para obter os valores de peso de _house_ basta olhar na linha 958 da tabela de pesos. Esse processo é conhecido como **_embedding lookup_** e o número de neurônios dessa camada oculta é conhecida como **_embedding dimension_**.\n",
    "\n",
    "Embeddings não são usados para palavras, apenas. Você pode usá-los para qualquer modelo que tenha um número massivo de classe. Um desses modelos é o _Word2Vec_, que usa modelos de palavras para encontrar representações semânticas das palavras.\n",
    "\n",
    "## A intuição do Word2Vec\n",
    "\n",
    "O _Word2Vec_ foi uma aplicação capaz de mostrar que redes neurais conseguem encontrar assimilações entre palavras. Por exemplo:\n",
    "- **homem** está para **mulher** assim como **rei** está para?\n",
    "\n",
    "### Para pensar um pouco:\n",
    "\n",
    "Em seu paper original, Mikolov usou o dataset do _Google News_ que era composto de 3 milhões de palavras e frases, resultando em mais de 100 bilhões de palavras. Nele, foi possível encontrar associações bem interessantes, como:\n",
    "\n",
    "- Microsoft - Windows ; Google- ?; IBM- ?; Apple -?\n",
    "\n",
    "![Word2Vec](img/word2vec.png)\n",
    "\n",
    "### Por debaixo dos panos\n",
    "\n",
    "![SemanticSpace](img/semanticspace.png)\n",
    "\n",
    "\n",
    "Sendo assim, é possível encontrar relações bem interessantes, não só limitadas a relações pessoais, como também de verbos e lugares:\n",
    "\n",
    "\n",
    "![Relationships](img/relationships.png)\n",
    "\n",
    "\n",
    "### A arquitetura por trás disso\n",
    "\n",
    "A ideia aqui não é ensinar a como implementar o _Word2Vec_ do zero, mas sim entender toda a intuição por trás dele. Lembra que mais cedo eu falei que estruturas de _Embedding_ nada mais são do que camadas ocultas de Redes Neurais? Então, as duas arquiteturas que implementam o _Word2Vec_ são **Redes Neurais**: o continuous Bag of Words (**CBOW**) e o **Skip-gram**.\n",
    "\n",
    "![Architetures](img/arc.png)\n",
    "\n",
    "Como podemos ver, as diferenças entre cada uma das arquiteturas é a seguinte:\n",
    "\n",
    "-**CBOW**: Dado um contexto (uma janela de palavras), eu tento prever a palavra do meio da janela.\n",
    "\n",
    "-**Skip-gram**: Dada uma palavra, eu tento prever as palavras em volta (ou o contexto) em que ela está inserida.\n",
    "\n",
    "Pelo paper, a arquitetura _skipgram_ tende a performar melhor que o _CBOW_.\n",
    "\n",
    "# TODO\n",
    "\n",
    "\n",
    "- Dizer o que é o Most Similar\n",
    "- Falar de Levenisthein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando os dados\n",
    "\n",
    "Para a aula de hoje, usaremos apenas o pacote de processamento de texto `gensim`. A ideia é mostrar que ele é uma alternativa ao `scikit-learn` para algumas etapas de pré processamento, mas também é válido dizer que ambos são complementares. Em outras palavras, é possível usar a saída de algum processo do `gensim` no `scikit` e vice versa.\n",
    "\n",
    "Uma coisa legal do Word2Vec é que ele pode ser considerado como um tipo de aprendizado **não** surpervisionado. Em outras palavras, ele é capaz de aprender de textos que não possuam um rótulo (ou label). Para ilustrar isso, para o treinamento aqui, além de usarmos o _labeledTrainData.tsv_, também usaremos o _unlabeledTrainData.tsv_, que contém 50 mil reviews adicionais, sem nenhuma label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from files \n",
    "train = pd.read_csv( \"data/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "test = pd.read_csv( \"data/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( \"data/unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré Processando o texto\n",
    "\n",
    "É interessante realizarmos o pré processamento dos textos que vamos trabalhar, igual fizemos na aula passada. Contudo, diferente do modelo **BoW** que vimos, o _Word2Vec_ **leva** em consideração o contexto das palavras. Sendo assim, talvez não seja muito legal removermos as *stop words*, assim como reduzir as palavras ao seu radical (stemming ou lemma) pode não ser desejável.\n",
    "\n",
    "Contudo, um pré processamento interessante é remover as URL's, uma vez que a chance de um site ser citado em um review é baixo e não estamos muito interessados na relação entre sites, mas no contexto em que levou a pessoa a citar alguma fonte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#exemplo de uma regex não trivial\n",
    "#RE_URL = re.compile(r'(http[s]?://)?(www)?.([a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\\.(com|net|org)(.[A-Za-z]{2})?')\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    \n",
    "    # 1. Ignore URLs\n",
    "    review_text = re.sub(\"(?:https?):\\/\\/[\\n\\S]+\",\"<URL>\", review)\n",
    "    # 2. Remove HTML\n",
    "    review_text = BeautifulSoup(review_text, \"html5lib\").get_text()\n",
    "    #  \n",
    "    # 3. Remove non-letters\n",
    "    review_text = re.sub(\"\\W+\",\" \", review_text)\n",
    "\n",
    "\n",
    "    #\n",
    "    # 4. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 5. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 6. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez limpa, agora precisamos converter os nossos dados para o tipo de dado que a biblioteca do `gensim` aceite. O _Word2Vec_ recebe como entrada sentenças únicas, cada uma formada por uma lista de palavras. Ou seja, precisamos de uma lista de listas.\n",
    "\n",
    "Definir onde começa ou onde acaba uma sentença não é algo muito claro. Existem sentenças que podem terminar com \"?\";\".\";\"!\" entre outras. Por conta disso, usaremos o pacote punkt tokenizer do NLTK para fazer esse tipo de split pra gente. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review,remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = sent_tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/25000 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 6/25000 [00:00<06:58, 59.68it/s]\u001b[A\n",
      "  0%|          | 17/25000 [00:00<06:10, 67.51it/s]\u001b[A\n",
      "  0%|          | 22/25000 [00:00<08:07, 51.26it/s]\u001b[A\n",
      "  0%|          | 29/25000 [00:00<07:32, 55.24it/s]\u001b[A\n",
      "  0%|          | 36/25000 [00:00<07:06, 58.59it/s]\u001b[A\n",
      "  0%|          | 45/25000 [00:00<06:30, 63.85it/s]\u001b[A\n",
      "  0%|          | 54/25000 [00:00<05:59, 69.43it/s]\u001b[A\n",
      "  0%|          | 61/25000 [00:00<05:59, 69.46it/s]\u001b[A\n",
      "  0%|          | 68/25000 [00:01<07:16, 57.15it/s]\u001b[A\n",
      "  0%|          | 78/25000 [00:01<06:21, 65.27it/s]\u001b[A\n",
      "  0%|          | 86/25000 [00:01<06:49, 60.77it/s]\u001b[A\n",
      "  0%|          | 95/25000 [00:01<06:13, 66.73it/s]\u001b[A\n",
      "  0%|          | 104/25000 [00:01<05:47, 71.55it/s]\u001b[A\n",
      "  0%|          | 112/25000 [00:01<06:34, 63.12it/s]\u001b[A\n",
      "  0%|          | 121/25000 [00:01<05:59, 69.25it/s]\u001b[A\n",
      "  1%|          | 129/25000 [00:01<06:38, 62.43it/s]\u001b[A\n",
      "  1%|          | 137/25000 [00:02<06:15, 66.16it/s]\u001b[A\n",
      "  1%|          | 145/25000 [00:02<06:18, 65.62it/s]\u001b[A\n",
      "  1%|          | 152/25000 [00:02<06:49, 60.67it/s]\u001b[A\n",
      "  1%|          | 160/25000 [00:02<06:25, 64.46it/s]\u001b[A\n",
      "  1%|          | 167/25000 [00:02<07:09, 57.86it/s]\u001b[A\n",
      "  1%|          | 174/25000 [00:02<06:58, 59.32it/s]\u001b[A\n",
      "  1%|          | 181/25000 [00:02<06:40, 61.94it/s]\u001b[A\n",
      "  1%|          | 188/25000 [00:02<07:49, 52.80it/s]\u001b[A\n",
      "  1%|          | 198/25000 [00:03<06:44, 61.33it/s]\u001b[A\n",
      "  1%|          | 205/25000 [00:03<06:39, 62.02it/s]\u001b[A\n",
      "  1%|          | 212/25000 [00:03<06:29, 63.72it/s]\u001b[A\n",
      "  1%|          | 219/25000 [00:03<06:41, 61.65it/s]\u001b[A/Users/abarbosa/miniconda3/envs/somostera/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "\n",
      "  1%|          | 226/25000 [00:03<06:39, 61.96it/s]\u001b[A\n",
      "  1%|          | 233/25000 [00:03<08:05, 51.05it/s]\u001b[A\n",
      "  1%|          | 239/25000 [00:03<07:55, 52.02it/s]\u001b[A\n",
      "  1%|          | 245/25000 [00:03<07:39, 53.88it/s]\u001b[A\n",
      "  1%|          | 253/25000 [00:04<06:55, 59.61it/s]\u001b[A\n",
      "  1%|          | 260/25000 [00:04<09:00, 45.78it/s]\u001b[A\n",
      "  1%|          | 267/25000 [00:04<08:29, 48.57it/s]\u001b[A\n",
      "  1%|          | 274/25000 [00:04<07:51, 52.44it/s]\u001b[A\n",
      "  1%|          | 282/25000 [00:04<07:09, 57.58it/s]\u001b[A\n",
      "  1%|          | 291/25000 [00:04<06:26, 63.85it/s]\u001b[A\n",
      "  1%|          | 299/25000 [00:04<06:13, 66.16it/s]\u001b[A\n",
      "  1%|          | 307/25000 [00:04<06:54, 59.50it/s]\u001b[A\n",
      "  1%|▏         | 314/25000 [00:05<06:49, 60.31it/s]\u001b[A\n",
      "  1%|▏         | 323/25000 [00:05<06:13, 66.03it/s]\u001b[A\n",
      "  1%|▏         | 331/25000 [00:05<06:04, 67.76it/s]\u001b[A\n",
      "  1%|▏         | 339/25000 [00:05<07:10, 57.30it/s]\u001b[A\n",
      "  1%|▏         | 346/25000 [00:05<08:18, 49.48it/s]\u001b[A\n",
      "  1%|▏         | 356/25000 [00:05<07:17, 56.27it/s]\u001b[A\n",
      "  1%|▏         | 364/25000 [00:05<06:52, 59.74it/s]\u001b[A\n",
      "  1%|▏         | 373/25000 [00:06<06:13, 65.97it/s]\u001b[A\n",
      "  2%|▏         | 381/25000 [00:06<07:22, 55.60it/s]\u001b[A\n",
      "  2%|▏         | 388/25000 [00:06<08:10, 50.19it/s]\u001b[A\n",
      "  2%|▏         | 394/25000 [00:06<07:47, 52.68it/s]\u001b[A\n",
      "  2%|▏         | 402/25000 [00:06<07:03, 58.14it/s]\u001b[A\n",
      "  2%|▏         | 409/25000 [00:06<06:51, 59.82it/s]\u001b[A\n",
      "  2%|▏         | 416/25000 [00:06<06:36, 62.00it/s]\u001b[A\n",
      "  2%|▏         | 423/25000 [00:06<07:26, 55.00it/s]\u001b[A\n",
      "  2%|▏         | 429/25000 [00:07<07:37, 53.72it/s]\u001b[A\n",
      "  2%|▏         | 435/25000 [00:07<11:25, 35.81it/s]\u001b[A\n",
      "  2%|▏         | 441/25000 [00:07<10:24, 39.30it/s]\u001b[A\n",
      "  2%|▏         | 446/25000 [00:07<09:53, 41.35it/s]\u001b[A\n",
      "  2%|▏         | 458/25000 [00:07<09:21, 43.69it/s]\u001b[A\n",
      "100%|██████████| 25000/25000 [07:09<00:00, 58.26it/s]\n",
      "  0%|          | 9/50000 [00:00<09:21, 89.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Unlabeled Sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31190/50000 [09:15<15:36, 20.09it/s]/Users/abarbosa/miniconda3/envs/somostera/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "100%|██████████| 50000/50000 [14:57<00:00, 55.74it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "for review in tqdm(train[\"review\"], total=len(train[\"review\"])):\n",
    "    #fezer com extend e comparar\n",
    "    sentences += review_to_sentences(review)\n",
    "\n",
    "print (\"Starting Unlabeled Sentences\")\n",
    "\n",
    "for review in tqdm(unlabeled_train[\"review\"], total=len(unlabeled_train[\"review\"])):\n",
    "    sentences += review_to_sentences(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "795538"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with',\n",
       " 'all',\n",
       " 'this',\n",
       " 'stuff',\n",
       " 'going',\n",
       " 'down',\n",
       " 'at',\n",
       " 'the',\n",
       " 'moment',\n",
       " 'with',\n",
       " 'mj',\n",
       " 'i',\n",
       " 've',\n",
       " 'started',\n",
       " 'listening',\n",
       " 'to',\n",
       " 'his',\n",
       " 'music',\n",
       " 'watching',\n",
       " 'the',\n",
       " 'odd',\n",
       " 'documentary',\n",
       " 'here',\n",
       " 'and',\n",
       " 'there',\n",
       " 'watched',\n",
       " 'the',\n",
       " 'wiz',\n",
       " 'and',\n",
       " 'watched',\n",
       " 'moonwalker',\n",
       " 'again']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['maybe',\n",
       " 'i',\n",
       " 'just',\n",
       " 'want',\n",
       " 'to',\n",
       " 'get',\n",
       " 'a',\n",
       " 'certain',\n",
       " 'insight',\n",
       " 'into',\n",
       " 'this',\n",
       " 'guy',\n",
       " 'who',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'was',\n",
       " 'really',\n",
       " 'cool',\n",
       " 'in',\n",
       " 'the',\n",
       " 'eighties',\n",
       " 'just',\n",
       " 'to',\n",
       " 'maybe',\n",
       " 'make',\n",
       " 'up',\n",
       " 'my',\n",
       " 'mind',\n",
       " 'whether',\n",
       " 'he',\n",
       " 'is',\n",
       " 'guilty',\n",
       " 'or',\n",
       " 'innocent']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o modelo Word2Vec\n",
    "\n",
    "Com a lista de sentenças devidamente parseada, basta treinarmos o nosso modelo. Aqui tem uma lista dos parâmetros especificando o que cada uma coisa faz.\n",
    "\n",
    "- **Architecture**: As opções de arquitetura são os algoritmos possíveis para a realização do Word2Vec. As opções aqui são o _skip-gram_ (default) e o _continuous bag of words_ (_cbow_).\n",
    "\n",
    "- **Training algorithm**: _Hierarchical softmax_ (default) ou _negative sampling_. Aqui, o default funcionou bem.\n",
    "\n",
    "- **Downsampling of frequent words**: A documentação do Google sugere valores entre .00001 and .001. Aqui, algo epor volta de 0.001 parece que melhorou o modelo final.\n",
    "\n",
    "- **Word vector dimensionality**: Aqui a ideia de _feature_ é parecida com o modelo que vimos do _BoW_, mas cada coluna não necessariamente é uma palavra. Logo, mais _features_ podem resultar em tempos maiores de processamento, mas nem sempre em modelos ideiais. O default aqui é 300.\n",
    "\n",
    "- **Context / window size**: Quantas janelas de contexto o algoritmo de treinamento deve levar em consideração. 10 parece funcionar bem para o softmax hierarquico (quanto mais melhor, até certo ponto).\n",
    "\n",
    "- **Worker threads**: Numero de processos para correrem em paraleolo. Isso varia de máquina para máquina, mas algo entre 4 e 6 deve funcionar na maioria dos sistemas.\n",
    "\n",
    "- **Minimum word count**: Isso ajuda a limitar o tamanho do vocabulario de palavras significativas. Qualquer palavra que não ocorre ao menos esse número de vezes considerando **todos** os documentos é ignorada. Valores razoáveis são entre 10 e 100. Nesse caso, dado que cada filme ocorre 30 vezes, o valor setado de _minimum word count_ é de 40. Quanto maior o vocabulário, maior o tempo de execução é impactado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-28 23:27:37,159 : INFO : collecting all words and their counts\n",
      "2017-11-28 23:27:37,161 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-11-28 23:27:37,294 : INFO : PROGRESS: at sentence #10000, processed 227183 words, keeping 18050 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-28 23:27:37,386 : INFO : PROGRESS: at sentence #20000, processed 454403 words, keeping 25345 word types\n",
      "2017-11-28 23:27:37,478 : INFO : PROGRESS: at sentence #30000, processed 675010 words, keeping 30508 word types\n",
      "2017-11-28 23:27:37,562 : INFO : PROGRESS: at sentence #40000, processed 902687 words, keeping 34903 word types\n",
      "2017-11-28 23:27:37,631 : INFO : PROGRESS: at sentence #50000, processed 1123085 words, keeping 38385 word types\n",
      "2017-11-28 23:27:37,699 : INFO : PROGRESS: at sentence #60000, processed 1345767 words, keeping 41405 word types\n",
      "2017-11-28 23:27:37,765 : INFO : PROGRESS: at sentence #70000, processed 1570193 words, keeping 44067 word types\n",
      "2017-11-28 23:27:37,836 : INFO : PROGRESS: at sentence #80000, processed 1790610 words, keeping 46494 word types\n",
      "2017-11-28 23:27:37,909 : INFO : PROGRESS: at sentence #90000, processed 2016019 words, keeping 48972 word types\n",
      "2017-11-28 23:27:37,979 : INFO : PROGRESS: at sentence #100000, processed 2239092 words, keeping 51099 word types\n",
      "2017-11-28 23:27:38,066 : INFO : PROGRESS: at sentence #110000, processed 2460033 words, keeping 53028 word types\n",
      "2017-11-28 23:27:38,147 : INFO : PROGRESS: at sentence #120000, processed 2683309 words, keeping 55117 word types\n",
      "2017-11-28 23:27:38,237 : INFO : PROGRESS: at sentence #130000, processed 2910183 words, keeping 56913 word types\n",
      "2017-11-28 23:27:38,311 : INFO : PROGRESS: at sentence #140000, processed 3124127 words, keeping 58465 word types\n",
      "2017-11-28 23:27:38,405 : INFO : PROGRESS: at sentence #150000, processed 3350995 words, keeping 60237 word types\n",
      "2017-11-28 23:27:38,492 : INFO : PROGRESS: at sentence #160000, processed 3574834 words, keeping 61847 word types\n",
      "2017-11-28 23:27:38,581 : INFO : PROGRESS: at sentence #170000, processed 3799372 words, keeping 63350 word types\n",
      "2017-11-28 23:27:38,672 : INFO : PROGRESS: at sentence #180000, processed 4021142 words, keeping 64814 word types\n",
      "2017-11-28 23:27:38,765 : INFO : PROGRESS: at sentence #190000, processed 4247580 words, keeping 66139 word types\n",
      "2017-11-28 23:27:38,864 : INFO : PROGRESS: at sentence #200000, processed 4472913 words, keeping 67469 word types\n",
      "2017-11-28 23:27:38,952 : INFO : PROGRESS: at sentence #210000, processed 4695594 words, keeping 68811 word types\n",
      "2017-11-28 23:27:39,044 : INFO : PROGRESS: at sentence #220000, processed 4921860 words, keeping 70163 word types\n",
      "2017-11-28 23:27:39,126 : INFO : PROGRESS: at sentence #230000, processed 5145658 words, keeping 71452 word types\n",
      "2017-11-28 23:27:39,218 : INFO : PROGRESS: at sentence #240000, processed 5374409 words, keeping 72695 word types\n",
      "2017-11-28 23:27:39,307 : INFO : PROGRESS: at sentence #250000, processed 5589727 words, keeping 73923 word types\n",
      "2017-11-28 23:27:39,397 : INFO : PROGRESS: at sentence #260000, processed 5810887 words, keeping 75094 word types\n",
      "2017-11-28 23:27:39,494 : INFO : PROGRESS: at sentence #270000, processed 6033450 words, keeping 76440 word types\n",
      "2017-11-28 23:27:39,578 : INFO : PROGRESS: at sentence #280000, processed 6260506 words, keeping 78080 word types\n",
      "2017-11-28 23:27:39,678 : INFO : PROGRESS: at sentence #290000, processed 6484865 words, keeping 79588 word types\n",
      "2017-11-28 23:27:39,765 : INFO : PROGRESS: at sentence #300000, processed 6710734 words, keeping 80958 word types\n",
      "2017-11-28 23:27:39,856 : INFO : PROGRESS: at sentence #310000, processed 6937218 words, keeping 82315 word types\n",
      "2017-11-28 23:27:39,936 : INFO : PROGRESS: at sentence #320000, processed 7163316 words, keeping 83687 word types\n",
      "2017-11-28 23:27:40,007 : INFO : PROGRESS: at sentence #330000, processed 7386325 words, keeping 84974 word types\n",
      "2017-11-28 23:27:40,083 : INFO : PROGRESS: at sentence #340000, processed 7617027 words, keeping 86257 word types\n",
      "2017-11-28 23:27:40,161 : INFO : PROGRESS: at sentence #350000, processed 7841544 words, keeping 87452 word types\n",
      "2017-11-28 23:27:40,237 : INFO : PROGRESS: at sentence #360000, processed 8063329 words, keeping 88653 word types\n",
      "2017-11-28 23:27:40,314 : INFO : PROGRESS: at sentence #370000, processed 8291698 words, keeping 89807 word types\n",
      "2017-11-28 23:27:40,396 : INFO : PROGRESS: at sentence #380000, processed 8518070 words, keeping 91023 word types\n",
      "2017-11-28 23:27:40,483 : INFO : PROGRESS: at sentence #390000, processed 8749088 words, keeping 92101 word types\n",
      "2017-11-28 23:27:40,568 : INFO : PROGRESS: at sentence #400000, processed 8973214 words, keeping 93161 word types\n",
      "2017-11-28 23:27:40,645 : INFO : PROGRESS: at sentence #410000, processed 9195775 words, keeping 94154 word types\n",
      "2017-11-28 23:27:40,728 : INFO : PROGRESS: at sentence #420000, processed 9418110 words, keeping 95222 word types\n",
      "2017-11-28 23:27:40,837 : INFO : PROGRESS: at sentence #430000, processed 9646985 words, keeping 96274 word types\n",
      "2017-11-28 23:27:40,954 : INFO : PROGRESS: at sentence #440000, processed 9875019 words, keeping 97287 word types\n",
      "2017-11-28 23:27:41,051 : INFO : PROGRESS: at sentence #450000, processed 10099927 words, keeping 98452 word types\n",
      "2017-11-28 23:27:41,154 : INFO : PROGRESS: at sentence #460000, processed 10333974 words, keeping 99536 word types\n",
      "2017-11-28 23:27:41,245 : INFO : PROGRESS: at sentence #470000, processed 10563044 words, keeping 100419 word types\n",
      "2017-11-28 23:27:41,322 : INFO : PROGRESS: at sentence #480000, processed 10784580 words, keeping 101382 word types\n",
      "2017-11-28 23:27:41,404 : INFO : PROGRESS: at sentence #490000, processed 11012597 words, keeping 102450 word types\n",
      "2017-11-28 23:27:41,490 : INFO : PROGRESS: at sentence #500000, processed 11235494 words, keeping 103380 word types\n",
      "2017-11-28 23:27:41,576 : INFO : PROGRESS: at sentence #510000, processed 11462005 words, keeping 104342 word types\n",
      "2017-11-28 23:27:41,653 : INFO : PROGRESS: at sentence #520000, processed 11686658 words, keeping 105264 word types\n",
      "2017-11-28 23:27:41,726 : INFO : PROGRESS: at sentence #530000, processed 11912305 words, keeping 106111 word types\n",
      "2017-11-28 23:27:41,817 : INFO : PROGRESS: at sentence #540000, processed 12138211 words, keeping 107009 word types\n",
      "2017-11-28 23:27:41,903 : INFO : PROGRESS: at sentence #550000, processed 12365006 words, keeping 107906 word types\n",
      "2017-11-28 23:27:42,008 : INFO : PROGRESS: at sentence #560000, processed 12587484 words, keeping 108794 word types\n",
      "2017-11-28 23:27:42,119 : INFO : PROGRESS: at sentence #570000, processed 12817861 words, keeping 109608 word types\n",
      "2017-11-28 23:27:42,229 : INFO : PROGRESS: at sentence #580000, processed 13040564 words, keeping 110503 word types\n",
      "2017-11-28 23:27:42,340 : INFO : PROGRESS: at sentence #590000, processed 13267197 words, keeping 111387 word types\n",
      "2017-11-28 23:27:42,456 : INFO : PROGRESS: at sentence #600000, processed 13490648 words, keeping 112149 word types\n",
      "2017-11-28 23:27:42,568 : INFO : PROGRESS: at sentence #610000, processed 13712854 words, keeping 113057 word types\n",
      "2017-11-28 23:27:42,690 : INFO : PROGRESS: at sentence #620000, processed 13940522 words, keeping 113843 word types\n",
      "2017-11-28 23:27:42,800 : INFO : PROGRESS: at sentence #630000, processed 14166025 words, keeping 114653 word types\n",
      "2017-11-28 23:27:42,908 : INFO : PROGRESS: at sentence #640000, processed 14387994 words, keeping 115502 word types\n",
      "2017-11-28 23:27:43,025 : INFO : PROGRESS: at sentence #650000, processed 14615128 words, keeping 116322 word types\n",
      "2017-11-28 23:27:43,135 : INFO : PROGRESS: at sentence #660000, processed 14839125 words, keeping 117111 word types\n",
      "2017-11-28 23:27:43,245 : INFO : PROGRESS: at sentence #670000, processed 15063726 words, keeping 117841 word types\n",
      "2017-11-28 23:27:43,331 : INFO : PROGRESS: at sentence #680000, processed 15289807 words, keeping 118573 word types\n",
      "2017-11-28 23:27:43,416 : INFO : PROGRESS: at sentence #690000, processed 15513127 words, keeping 119384 word types\n",
      "2017-11-28 23:27:43,502 : INFO : PROGRESS: at sentence #700000, processed 15742984 words, keeping 120226 word types\n",
      "2017-11-28 23:27:43,591 : INFO : PROGRESS: at sentence #710000, processed 15967181 words, keeping 120895 word types\n",
      "2017-11-28 23:27:43,676 : INFO : PROGRESS: at sentence #720000, processed 16193688 words, keeping 121537 word types\n",
      "2017-11-28 23:27:43,768 : INFO : PROGRESS: at sentence #730000, processed 16421235 words, keeping 122286 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-28 23:27:43,866 : INFO : PROGRESS: at sentence #740000, processed 16643523 words, keeping 123026 word types\n",
      "2017-11-28 23:27:43,962 : INFO : PROGRESS: at sentence #750000, processed 16863120 words, keeping 123677 word types\n",
      "2017-11-28 23:27:44,047 : INFO : PROGRESS: at sentence #760000, processed 17083737 words, keeping 124327 word types\n",
      "2017-11-28 23:27:44,131 : INFO : PROGRESS: at sentence #770000, processed 17312165 words, keeping 125128 word types\n",
      "2017-11-28 23:27:44,225 : INFO : PROGRESS: at sentence #780000, processed 17543573 words, keeping 125864 word types\n",
      "2017-11-28 23:27:44,302 : INFO : PROGRESS: at sentence #790000, processed 17771837 words, keeping 126562 word types\n",
      "2017-11-28 23:27:44,342 : INFO : collected 127017 word types from a corpus of 17895591 raw words and 795538 sentences\n",
      "2017-11-28 23:27:44,343 : INFO : Loading a fresh vocabulary\n",
      "2017-11-28 23:27:44,458 : INFO : min_count=40 retains 16717 unique words (13% of original 127017, drops 110300)\n",
      "2017-11-28 23:27:44,459 : INFO : min_count=40 leaves 17328137 word corpus (96% of original 17895591, drops 567454)\n",
      "2017-11-28 23:27:44,520 : INFO : deleting the raw counts dictionary of 127017 items\n",
      "2017-11-28 23:27:44,527 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2017-11-28 23:27:44,527 : INFO : downsampling leaves estimated 12855820 word corpus (74.2% of prior 17328137)\n",
      "2017-11-28 23:27:44,529 : INFO : estimated required memory for 16717 words and 300 dimensions: 48479300 bytes\n",
      "2017-11-28 23:27:44,601 : INFO : resetting layer weights\n",
      "2017-11-28 23:27:44,912 : INFO : training model with 4 workers on 16717 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-28 23:27:45,924 : INFO : PROGRESS: at 1.51% examples, 966859 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:27:46,925 : INFO : PROGRESS: at 2.95% examples, 942283 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:27:47,936 : INFO : PROGRESS: at 4.27% examples, 904545 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:27:48,940 : INFO : PROGRESS: at 5.50% examples, 874395 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:27:49,945 : INFO : PROGRESS: at 6.81% examples, 864903 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:27:50,947 : INFO : PROGRESS: at 8.38% examples, 890157 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:27:51,948 : INFO : PROGRESS: at 9.81% examples, 895016 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:27:52,951 : INFO : PROGRESS: at 11.46% examples, 915355 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:27:53,955 : INFO : PROGRESS: at 12.95% examples, 920780 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:27:54,956 : INFO : PROGRESS: at 14.59% examples, 934032 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:27:55,967 : INFO : PROGRESS: at 15.72% examples, 914711 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:27:56,970 : INFO : PROGRESS: at 16.74% examples, 892776 words/s, in_qsize 6, out_qsize 1\n",
      "2017-11-28 23:27:57,976 : INFO : PROGRESS: at 18.15% examples, 893739 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:27:58,976 : INFO : PROGRESS: at 19.56% examples, 894354 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:27:59,977 : INFO : PROGRESS: at 21.03% examples, 898204 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:00,980 : INFO : PROGRESS: at 22.56% examples, 902433 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:01,982 : INFO : PROGRESS: at 24.14% examples, 908256 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:02,985 : INFO : PROGRESS: at 25.68% examples, 912520 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:03,997 : INFO : PROGRESS: at 27.15% examples, 913301 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:05,003 : INFO : PROGRESS: at 28.91% examples, 924010 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:06,005 : INFO : PROGRESS: at 30.66% examples, 933792 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:07,008 : INFO : PROGRESS: at 32.37% examples, 941702 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:08,010 : INFO : PROGRESS: at 33.99% examples, 945837 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:09,018 : INFO : PROGRESS: at 35.69% examples, 951830 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:10,026 : INFO : PROGRESS: at 37.39% examples, 957004 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:11,034 : INFO : PROGRESS: at 38.96% examples, 958797 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:12,036 : INFO : PROGRESS: at 40.26% examples, 954280 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:13,041 : INFO : PROGRESS: at 41.36% examples, 945401 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:14,048 : INFO : PROGRESS: at 42.56% examples, 938798 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:15,059 : INFO : PROGRESS: at 43.88% examples, 935155 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:16,063 : INFO : PROGRESS: at 45.32% examples, 934705 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:17,067 : INFO : PROGRESS: at 46.79% examples, 934722 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:18,070 : INFO : PROGRESS: at 48.32% examples, 936083 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:19,075 : INFO : PROGRESS: at 49.36% examples, 928281 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:20,081 : INFO : PROGRESS: at 50.46% examples, 921903 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:21,083 : INFO : PROGRESS: at 51.96% examples, 923285 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:22,085 : INFO : PROGRESS: at 53.44% examples, 924076 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:23,094 : INFO : PROGRESS: at 55.00% examples, 925931 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:24,106 : INFO : PROGRESS: at 56.56% examples, 927647 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:25,108 : INFO : PROGRESS: at 58.13% examples, 929676 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:26,111 : INFO : PROGRESS: at 59.68% examples, 931227 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:27,118 : INFO : PROGRESS: at 61.21% examples, 932453 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:28,120 : INFO : PROGRESS: at 62.76% examples, 933586 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:29,120 : INFO : PROGRESS: at 64.31% examples, 934835 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:30,125 : INFO : PROGRESS: at 65.85% examples, 935924 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:31,129 : INFO : PROGRESS: at 67.36% examples, 936378 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:32,130 : INFO : PROGRESS: at 68.87% examples, 937184 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:33,137 : INFO : PROGRESS: at 70.38% examples, 937847 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:34,141 : INFO : PROGRESS: at 71.90% examples, 938791 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:35,144 : INFO : PROGRESS: at 73.43% examples, 939610 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:36,145 : INFO : PROGRESS: at 74.97% examples, 940587 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:37,149 : INFO : PROGRESS: at 76.52% examples, 941574 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:38,150 : INFO : PROGRESS: at 78.05% examples, 942464 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:39,154 : INFO : PROGRESS: at 79.59% examples, 943277 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:40,155 : INFO : PROGRESS: at 81.11% examples, 943950 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:41,163 : INFO : PROGRESS: at 82.67% examples, 944640 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:42,167 : INFO : PROGRESS: at 84.22% examples, 945353 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:43,173 : INFO : PROGRESS: at 85.77% examples, 946010 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-28 23:28:44,177 : INFO : PROGRESS: at 87.31% examples, 946533 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:45,178 : INFO : PROGRESS: at 88.80% examples, 946889 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:46,180 : INFO : PROGRESS: at 90.30% examples, 947219 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:47,182 : INFO : PROGRESS: at 91.81% examples, 947731 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:48,187 : INFO : PROGRESS: at 93.34% examples, 948212 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:49,192 : INFO : PROGRESS: at 94.85% examples, 948562 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-28 23:28:50,197 : INFO : PROGRESS: at 96.37% examples, 948898 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:51,199 : INFO : PROGRESS: at 97.90% examples, 949367 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:52,207 : INFO : PROGRESS: at 99.43% examples, 949761 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-28 23:28:52,563 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-11-28 23:28:52,570 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-28 23:28:52,576 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-28 23:28:52,583 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-28 23:28:52,584 : INFO : training on 89477955 raw words (64279313 effective words) took 67.7s, 949978 effective words/s\n",
      "2017-11-28 23:28:52,585 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-11-28 23:28:52,763 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2017-11-28 23:28:52,765 : INFO : not storing attribute syn0norm\n",
      "2017-11-28 23:28:52,766 : INFO : not storing attribute cum_table\n",
      "2017-11-28 23:28:53,501 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print (\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorando os resultados\n",
    "\n",
    "A função `doesnt_match` tentará deduzir qual palavra em um set é a mais _distinta_ das outras.\n",
    "\n",
    "De forma análoga, a função `most_similar` tentará deduzir qual palavra em um set é a mais _semelhante_ das outras.\n",
    "\n",
    "\n",
    "## Mas como definir similaridade? Que uma palavra é similar à outra?\n",
    "\n",
    "A métrica mais famosa para isso é a _Similaridade de Coscenos_ ou _Cosine Similarity_.\n",
    "\n",
    "Vamos voltar à definição de espaço semantico dada no começo da aula.\n",
    "\n",
    "![Male-Female](img/male-female.png)\n",
    "\n",
    "Olhando um pouco debaixo dos panos (porque na situação real é como se estivessemos olhando para um vetor de 300 dimensões, o que é muito abstrato), vamos projetar **apenas** essa situação de _Male-Female_\n",
    "\n",
    "\n",
    "![Male-Female](img/sematic-male-female.png)\n",
    "\n",
    "\n",
    "Ao olharmos esse espaço, vemos claramente que as palavras semânticamente próximas a _male_ também são próximas entre si. O mesmo acontece com _female_. Mas como que a gente encontra esse *valor*? Basta calcularmos o **cosceno** entre os vetores que formam essas palavras !\n",
    "\n",
    "Lembrando um pouquinho de matemática da faculdade, dado o produto escalar entre dois vetores e suas normas eu consigo encontrar o cosceno formado entre os dois:\n",
    "\n",
    "$A \\cdot B = \\lVert A \\rVert \\lVert B \\rVert \\cos\\theta   $\n",
    "\n",
    "![Cosine](img/cosine-similarity.png)\n",
    "\n",
    "\n",
    "Em NLP essa métrica é muito comum quando queremos calcular a similaridade entre documentos.\n",
    "\n",
    "## Entendendo a representação numérica das palavras\n",
    "\n",
    "Agora nós treinamos um modelo que consegue entender a relação semântica entre as palavras. Para algumas aplicações é exatamente isso que a gente quer, mas o que podemos fazer com isso? Os vetores de palavras criados pelo nosso modelo foram armazenados em um array `numpy` chamado `syn0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-28 23:36:08,530 : INFO : loading Word2Vec object from 300features_40minwords_10context\n",
      "2017-11-28 23:36:08,887 : INFO : loading wv recursively from 300features_40minwords_10context.wv.* with mmap=None\n",
      "2017-11-28 23:36:08,888 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-11-28 23:36:08,889 : INFO : setting ignored attribute cum_table to None\n",
      "2017-11-28 23:36:08,890 : INFO : loaded 300features_40minwords_10context\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec \n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")\n",
    "type(model.wv.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16717, 300)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembre-se que a gente limitou o número de ocorrências de palavras para um mínimo de 40. Isso justifica a menor quantidade de vocabulário, sendo formado por 16717 palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -3.10837384e-02,   4.03764322e-02,   4.10359241e-02,\n",
       "         5.15633961e-03,  -5.40248640e-02,  -7.78134391e-02,\n",
       "         1.35028914e-01,   7.21310675e-02,   3.28026302e-02,\n",
       "         2.29188558e-02,   2.05275626e-03,   3.21877468e-03,\n",
       "         3.96432392e-02,  -1.32296890e-01,   1.10449947e-01,\n",
       "        -3.37153859e-02,  -7.52574503e-02,  -8.52601156e-02,\n",
       "        -8.54553189e-03,   4.08086330e-02,   4.99160737e-02,\n",
       "        -5.79162985e-02,  -4.89986548e-03,  -5.31144291e-02,\n",
       "         1.21043175e-01,  -4.36744876e-02,  -2.73520648e-02,\n",
       "        -5.54944053e-02,  -3.95381078e-02,  -4.12611589e-02,\n",
       "         1.68824494e-02,  -3.27361412e-02,   8.19914192e-02,\n",
       "         2.42416244e-02,   3.13074738e-02,   6.38195872e-02,\n",
       "        -4.22389247e-02,   1.01815984e-02,  -7.76159763e-02,\n",
       "         8.76649246e-02,   1.06198631e-01,  -7.59951025e-02,\n",
       "        -1.18816376e-03,  -8.79209787e-02,  -4.08728644e-02,\n",
       "         5.11247665e-02,   8.62793345e-03,   4.73641492e-02,\n",
       "         3.71092930e-02,  -1.05294205e-01,  -5.26723862e-02,\n",
       "        -4.03785221e-02,  -1.46815330e-01,  -3.04856263e-02,\n",
       "         1.15584778e-02,  -5.14178351e-02,  -9.67727453e-02,\n",
       "         2.33198330e-02,  -5.79768941e-02,   1.50603294e-01,\n",
       "        -1.00633480e-01,  -3.33304554e-02,   8.67981687e-02,\n",
       "         6.95997253e-02,   2.56304182e-02,   7.00201392e-02,\n",
       "         9.50699970e-02,   2.29502674e-02,   6.27720729e-02,\n",
       "        -2.08706385e-03,   2.45998129e-02,   5.97059466e-02,\n",
       "        -1.29229762e-03,   1.48764700e-01,  -7.32637895e-03,\n",
       "        -7.42225954e-03,  -1.57040227e-02,   3.53998318e-02,\n",
       "         1.21890076e-01,  -1.02413580e-01,   6.77987263e-02,\n",
       "        -1.89850014e-02,  -6.69885427e-02,   1.01751788e-02,\n",
       "        -4.03802888e-03,  -1.30855963e-02,  -7.30780140e-02,\n",
       "         2.79089790e-02,   2.68924679e-03,   9.05333683e-02,\n",
       "         7.31564313e-02,  -7.71708926e-03,   3.31185833e-02,\n",
       "        -7.26070139e-05,   1.64244659e-02,  -4.86444049e-02,\n",
       "        -4.59713191e-02,   3.85878831e-02,  -4.77632992e-02,\n",
       "        -3.85042578e-02,  -4.10169102e-02,  -4.28946465e-02,\n",
       "         4.15141461e-03,   5.27253971e-02,   1.51404832e-02,\n",
       "        -4.04675193e-02,  -3.46314488e-03,  -1.12302704e-02,\n",
       "         1.23766758e-01,   8.13135803e-02,   5.51157966e-02,\n",
       "        -1.01990566e-01,  -3.98413744e-03,  -8.26213285e-02,\n",
       "        -4.41952609e-02,  -5.49589917e-02,  -1.43089488e-01,\n",
       "         4.39709201e-02,   8.83956626e-02,   8.64286050e-02,\n",
       "        -7.54888076e-03,  -6.47270903e-02,   5.88090811e-03,\n",
       "         1.34066224e-01,  -6.94401637e-02,  -2.75515523e-02,\n",
       "         1.05185889e-01,   3.20076980e-02,   4.13646661e-02,\n",
       "        -6.27242923e-02,  -1.13835158e-02,   9.72375870e-02,\n",
       "        -4.58148196e-02,  -4.82340828e-02,   5.12531064e-02,\n",
       "         3.38529535e-02,   7.30394665e-03,  -6.95030093e-02,\n",
       "        -9.21501964e-03,  -4.59939912e-02,  -2.10321750e-02,\n",
       "        -3.35470252e-02,   6.04491010e-02,   2.53957752e-02,\n",
       "        -6.07870258e-02,  -1.33537417e-02,  -7.05524832e-02,\n",
       "        -8.49383697e-02,   5.09523079e-02,   3.72159965e-02,\n",
       "         1.36524057e-02,  -5.25761843e-02,  -9.14430153e-03,\n",
       "         2.22177021e-02,  -1.52139934e-02,  -4.86273915e-02,\n",
       "         2.78427023e-02,   5.57241309e-03,   4.31581959e-02,\n",
       "        -3.78613956e-02,   1.51089281e-02,  -1.36978582e-01,\n",
       "        -3.60224135e-02,   7.17998445e-02,   2.29516309e-02,\n",
       "        -1.04196727e-01,   2.13665441e-02,   3.19920592e-02,\n",
       "        -8.74422640e-02,  -1.15999736e-01,   4.67651859e-02,\n",
       "         1.64716896e-02,   5.54257855e-02,   2.85449531e-02,\n",
       "         7.69302598e-04,   8.69459063e-02,  -7.92495720e-03,\n",
       "        -2.26042308e-02,   4.78984453e-02,  -1.06435701e-01,\n",
       "        -4.22156081e-02,   6.10764697e-02,  -1.83716863e-02,\n",
       "         1.49346013e-02,   6.32102787e-02,   6.96974061e-03,\n",
       "        -7.65736848e-02,  -2.35332679e-02,   1.65890250e-02,\n",
       "         1.04234502e-01,   1.04512006e-01,   3.77995409e-02,\n",
       "         5.54773025e-02,   4.59790602e-02,   2.86741536e-02,\n",
       "         9.32629779e-03,   1.33349687e-01,   5.35891764e-02,\n",
       "         5.61376028e-02,   1.58445798e-02,  -2.75715832e-02,\n",
       "         1.16315626e-01,  -2.86401883e-02,   2.78413910e-02,\n",
       "        -3.41010392e-02,   8.79961066e-03,  -2.94997022e-02,\n",
       "        -6.00767620e-02,   4.51394208e-02,   9.24292654e-02,\n",
       "        -2.73793750e-02,   3.23787779e-02,  -4.98282015e-02,\n",
       "         2.02603862e-02,   9.74840224e-02,   4.66717854e-02,\n",
       "        -2.09301393e-02,   8.00367910e-03,   2.79854611e-02,\n",
       "        -3.85014527e-02,   1.89472288e-02,   2.66233757e-02,\n",
       "        -8.42004865e-02,  -3.00848298e-02,   5.06800488e-02,\n",
       "         2.41409093e-02,   4.31467928e-02,  -5.56754209e-02,\n",
       "         4.38231677e-02,  -5.49109317e-02,   8.88697654e-02,\n",
       "        -8.21294114e-02,   3.19186151e-02,   6.88015968e-02,\n",
       "        -2.16024835e-02,  -1.21389818e-03,   1.69847254e-02,\n",
       "        -2.12577842e-02,  -4.42963801e-02,   6.01993799e-02,\n",
       "         1.66759491e-02,   1.62365418e-02,   1.59319248e-02,\n",
       "        -1.85288955e-02,  -1.19737890e-02,   2.82593053e-02,\n",
       "         1.67366266e-02,  -8.90404731e-02,  -6.35782548e-04,\n",
       "         1.84833519e-02,   4.06423360e-02,  -3.47231366e-02,\n",
       "         3.53276171e-02,   3.10338736e-02,   7.57435188e-02,\n",
       "        -3.40875052e-02,  -3.28242332e-02,  -7.88026527e-02,\n",
       "         8.72524902e-02,  -4.84032892e-02,   4.97456193e-02,\n",
       "        -3.05678956e-02,  -1.64670981e-02,   8.01440794e-03,\n",
       "        -3.94712351e-02,  -2.31211353e-02,  -6.69445470e-03,\n",
       "         7.06114471e-02,  -5.77718019e-03,   8.38544071e-02,\n",
       "         7.50244781e-02,  -9.67322439e-02,  -4.36751209e-02,\n",
       "         1.84613448e-02,   3.22197601e-02,   5.17883971e-02,\n",
       "        -6.68625236e-02,  -8.34876765e-03,  -6.16277754e-02,\n",
       "         8.95270798e-03,   2.68049221e-02,   1.26314044e-01,\n",
       "        -1.53313233e-02,  -1.81705505e-02,   1.27623696e-02,\n",
       "        -4.20725755e-02,  -5.63768893e-02,   3.43030058e-02,\n",
       "        -7.91865215e-02,   1.87939033e-02,   3.23484987e-02,\n",
       "         7.09513500e-02,   2.13585868e-02,   7.61415884e-02,\n",
       "         1.49712712e-02,   3.96804400e-02,  -4.50046584e-02,\n",
       "        -6.10457920e-02,  -7.69901425e-02,  -7.57189840e-02], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"flower\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model[\"flower\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De palavras para reviews \n",
    "\n",
    "Um problema que temos ao lidar com o dataset do IMDB é que os reviews tem tamanho variado. Precisaríamos, então, encontrar uma forma de pegar esses vetores de palavras e transformá-los em um conjunto de features que tem o mesmo tamanho para o mesmo _review_.\n",
    "\n",
    "Já que todas as palavras são representadas por um vetor de 300 dimensões, nós podemos usar operações vetoriais para combinar palavras. Por exemplo, podemos pegar todas as palavras que compõe um review e somá-las ou tirar a média. No caso, vamos tirar a média (contudo, aqui nós não precisamos mais do contexto das palavras e, então, podemos remover stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np  # Make sure that numpy is imported\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in tqdm(reviews,total=len(reviews)):\n",
    "\n",
    "       # \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [01:04<00:00, 388.75it/s]\n",
      "100%|██████████| 25000/25000 [00:32<00:00, 774.27it/s]\n",
      "  0%|          | 45/25000 [00:00<00:56, 443.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for test reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [01:00<00:00, 412.46it/s]\n",
      "100%|██████████| 25000/25000 [00:30<00:00, 807.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in tqdm(train[\"review\"], total=len(train[\"review\"])):\n",
    "    clean_train_reviews.append( review_to_wordlist( review, remove_stopwords=True ))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
    "\n",
    "print (\"Creating average feature vecs for test reviews\")\n",
    "clean_test_reviews = []\n",
    "for review in tqdm(test[\"review\"], total=len(test[\"review\"])):\n",
    "    clean_test_reviews.append( review_to_wordlist( review, remove_stopwords=True ))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui, como a ideia é usar o vetor de features como treino, estamos lidando novamente com um aprendizado **surpervisionado** e, então, usaremos apenas os dados com _label_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>\"Afraid of the Dark left me with the impressio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>\"A very accurate depiction of small time mob l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"2913_8\"</td>\n",
       "      <td>\"...as valuable as King Tut's tomb! (OK, maybe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"4396_1\"</td>\n",
       "      <td>\"This has to be one of the biggest misfires ev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"395_2\"</td>\n",
       "      <td>\"This is one of those movies I watched, and wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"10616_1\"</td>\n",
       "      <td>\"The worst movie i've seen in years (and i've ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"9074_9\"</td>\n",
       "      <td>\"Five medical students (Kevin Bacon, David Lab...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"9252_3\"</td>\n",
       "      <td>\"'The Mill on the Floss' was one of the lesser...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"9896_9\"</td>\n",
       "      <td>\"I just saw this film at the phoenix film fest...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\"574_4\"</td>\n",
       "      <td>\"\\\"The Love Letter\\\" is one of those movies th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"11182_8\"</td>\n",
       "      <td>\"Another fantastic offering from the Monkey Is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\"11656_4\"</td>\n",
       "      <td>\"This was included on the disk \\\"Shorts: Volum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"2322_4\"</td>\n",
       "      <td>\"I'm not really much of an Abbott &amp; Costello f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\"8703_1\"</td>\n",
       "      <td>\"This movie was dreadful. Biblically very inac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\"7483_1\"</td>\n",
       "      <td>\"I don't think I've ever gave something a 1/10...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\"6007_10\"</td>\n",
       "      <td>\"Excellent story-telling and cinematography. P...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\"12424_4\"</td>\n",
       "      <td>\"I completely forgot that I'd seen this within...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>\"4672_1\"</td>\n",
       "      <td>\"I like action movies. I have a softspot for \\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>\"10841_3\"</td>\n",
       "      <td>\"This is one of the worst Sandra Bullock movie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\"8954_7\"</td>\n",
       "      <td>\"Watched this flick on Saturday afternoon cabl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>\"7392_1\"</td>\n",
       "      <td>\"I went to see \\\"TKIA\\\" with high expectations...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>\"10288_8\"</td>\n",
       "      <td>\"All credit to writer/director Gilles Mimouni ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>\"5343_4\"</td>\n",
       "      <td>\"As a writing teacher, there are two ending I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>\"4950_1\"</td>\n",
       "      <td>\"I don't know why this has gotten any decent r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>\"9257_4\"</td>\n",
       "      <td>\"This film was released in the UK under the na...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>\"8689_3\"</td>\n",
       "      <td>\"Uncle Fred Olen Ray once again gives us a lit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>\"4480_2\"</td>\n",
       "      <td>\"OK, it's watchable if you are sick in bed or ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24970</th>\n",
       "      <td>\"6857_10\"</td>\n",
       "      <td>\"With \\\"Anatomy\\\" the german film producers ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24971</th>\n",
       "      <td>\"11091_8\"</td>\n",
       "      <td>\"This movie is one of my all-time favorites. I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24972</th>\n",
       "      <td>\"4167_2\"</td>\n",
       "      <td>\"I found Code 46 very disappointing. I thought...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24973</th>\n",
       "      <td>\"679_4\"</td>\n",
       "      <td>\"Tamara Anderson and her family are moving onc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24974</th>\n",
       "      <td>\"10147_1\"</td>\n",
       "      <td>\"Now I've seen it all. Just when I thought it ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24975</th>\n",
       "      <td>\"6875_1\"</td>\n",
       "      <td>\"In this movie everything possible was wrong a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24976</th>\n",
       "      <td>\"923_10\"</td>\n",
       "      <td>\"Well every scene so perfectly presented. Neve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24977</th>\n",
       "      <td>\"6200_8\"</td>\n",
       "      <td>\"Sleeper Cell is what 24 should have been. 24 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24978</th>\n",
       "      <td>\"7208_8\"</td>\n",
       "      <td>\"Not for everyone, but I really like it. Nice ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24979</th>\n",
       "      <td>\"5363_8\"</td>\n",
       "      <td>\"Set just before the Second World War, this is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24980</th>\n",
       "      <td>\"4067_8\"</td>\n",
       "      <td>\"Contains Spoiler The movie is a good action/c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24981</th>\n",
       "      <td>\"1773_7\"</td>\n",
       "      <td>\"This is one of several period sea-faring yarn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24982</th>\n",
       "      <td>\"1498_10\"</td>\n",
       "      <td>\"Hearkening back to those \\\"Good Old Days\\\" of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24983</th>\n",
       "      <td>\"10497_10\"</td>\n",
       "      <td>\"I thought this to be a pretty good example of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24984</th>\n",
       "      <td>\"3444_10\"</td>\n",
       "      <td>\"Seeing this film, or rather set of films, in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24985</th>\n",
       "      <td>\"588_2\"</td>\n",
       "      <td>\"I didn't like this movie for many reasons - V...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24986</th>\n",
       "      <td>\"9678_9\"</td>\n",
       "      <td>\"I absolutely love this show!!!!!!!, Its basic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24987</th>\n",
       "      <td>\"1983_9\"</td>\n",
       "      <td>\"eXistenZ combines director David Cronenberg's...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24988</th>\n",
       "      <td>\"5012_3\"</td>\n",
       "      <td>\"this movie is allegedly a comedy.so where did...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24989</th>\n",
       "      <td>\"12240_2\"</td>\n",
       "      <td>\"The Comebacks is a spoof on inspirational spo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24990</th>\n",
       "      <td>\"5071_2\"</td>\n",
       "      <td>\"I'd love to write a little summary of this mo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24991</th>\n",
       "      <td>\"5078_2\"</td>\n",
       "      <td>\"Obvious tailored vehicle for Ryan Philippe. I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24992</th>\n",
       "      <td>\"10069_3\"</td>\n",
       "      <td>\"&lt;br /&gt;&lt;br /&gt;JURASSIC PARK III *___ Adventure ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24993</th>\n",
       "      <td>\"7407_8\"</td>\n",
       "      <td>\"If you're even mildly interested in the War b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24994</th>\n",
       "      <td>\"7207_1\"</td>\n",
       "      <td>\"It used to be that video distributors like Su...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>\"2155_10\"</td>\n",
       "      <td>\"Sony Pictures Classics, I'm looking at you! S...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>\"59_10\"</td>\n",
       "      <td>\"I always felt that Ms. Merkerson had never go...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>\"2531_1\"</td>\n",
       "      <td>\"I was so disappointed in this movie. I am ver...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>\"7772_8\"</td>\n",
       "      <td>\"From the opening sequence, filled with black ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>\"11465_10\"</td>\n",
       "      <td>\"This is a great horror film for people who do...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                             review  \\\n",
       "0      \"12311_10\"  \"Naturally in a film who's main themes are of ...   \n",
       "1        \"8348_2\"  \"This movie is a disaster within a disaster fi...   \n",
       "2        \"5828_4\"  \"All in all, this is a movie for kids. We saw ...   \n",
       "3        \"7186_2\"  \"Afraid of the Dark left me with the impressio...   \n",
       "4       \"12128_7\"  \"A very accurate depiction of small time mob l...   \n",
       "5        \"2913_8\"  \"...as valuable as King Tut's tomb! (OK, maybe...   \n",
       "6        \"4396_1\"  \"This has to be one of the biggest misfires ev...   \n",
       "7         \"395_2\"  \"This is one of those movies I watched, and wo...   \n",
       "8       \"10616_1\"  \"The worst movie i've seen in years (and i've ...   \n",
       "9        \"9074_9\"  \"Five medical students (Kevin Bacon, David Lab...   \n",
       "10       \"9252_3\"  \"'The Mill on the Floss' was one of the lesser...   \n",
       "11       \"9896_9\"  \"I just saw this film at the phoenix film fest...   \n",
       "12        \"574_4\"  \"\\\"The Love Letter\\\" is one of those movies th...   \n",
       "13      \"11182_8\"  \"Another fantastic offering from the Monkey Is...   \n",
       "14      \"11656_4\"  \"This was included on the disk \\\"Shorts: Volum...   \n",
       "15       \"2322_4\"  \"I'm not really much of an Abbott & Costello f...   \n",
       "16       \"8703_1\"  \"This movie was dreadful. Biblically very inac...   \n",
       "17       \"7483_1\"  \"I don't think I've ever gave something a 1/10...   \n",
       "18      \"6007_10\"  \"Excellent story-telling and cinematography. P...   \n",
       "19      \"12424_4\"  \"I completely forgot that I'd seen this within...   \n",
       "20       \"4672_1\"  \"I like action movies. I have a softspot for \\...   \n",
       "21      \"10841_3\"  \"This is one of the worst Sandra Bullock movie...   \n",
       "22       \"8954_7\"  \"Watched this flick on Saturday afternoon cabl...   \n",
       "23       \"7392_1\"  \"I went to see \\\"TKIA\\\" with high expectations...   \n",
       "24      \"10288_8\"  \"All credit to writer/director Gilles Mimouni ...   \n",
       "25       \"5343_4\"  \"As a writing teacher, there are two ending I ...   \n",
       "26       \"4950_1\"  \"I don't know why this has gotten any decent r...   \n",
       "27       \"9257_4\"  \"This film was released in the UK under the na...   \n",
       "28       \"8689_3\"  \"Uncle Fred Olen Ray once again gives us a lit...   \n",
       "29       \"4480_2\"  \"OK, it's watchable if you are sick in bed or ...   \n",
       "...           ...                                                ...   \n",
       "24970   \"6857_10\"  \"With \\\"Anatomy\\\" the german film producers ha...   \n",
       "24971   \"11091_8\"  \"This movie is one of my all-time favorites. I...   \n",
       "24972    \"4167_2\"  \"I found Code 46 very disappointing. I thought...   \n",
       "24973     \"679_4\"  \"Tamara Anderson and her family are moving onc...   \n",
       "24974   \"10147_1\"  \"Now I've seen it all. Just when I thought it ...   \n",
       "24975    \"6875_1\"  \"In this movie everything possible was wrong a...   \n",
       "24976    \"923_10\"  \"Well every scene so perfectly presented. Neve...   \n",
       "24977    \"6200_8\"  \"Sleeper Cell is what 24 should have been. 24 ...   \n",
       "24978    \"7208_8\"  \"Not for everyone, but I really like it. Nice ...   \n",
       "24979    \"5363_8\"  \"Set just before the Second World War, this is...   \n",
       "24980    \"4067_8\"  \"Contains Spoiler The movie is a good action/c...   \n",
       "24981    \"1773_7\"  \"This is one of several period sea-faring yarn...   \n",
       "24982   \"1498_10\"  \"Hearkening back to those \\\"Good Old Days\\\" of...   \n",
       "24983  \"10497_10\"  \"I thought this to be a pretty good example of...   \n",
       "24984   \"3444_10\"  \"Seeing this film, or rather set of films, in ...   \n",
       "24985     \"588_2\"  \"I didn't like this movie for many reasons - V...   \n",
       "24986    \"9678_9\"  \"I absolutely love this show!!!!!!!, Its basic...   \n",
       "24987    \"1983_9\"  \"eXistenZ combines director David Cronenberg's...   \n",
       "24988    \"5012_3\"  \"this movie is allegedly a comedy.so where did...   \n",
       "24989   \"12240_2\"  \"The Comebacks is a spoof on inspirational spo...   \n",
       "24990    \"5071_2\"  \"I'd love to write a little summary of this mo...   \n",
       "24991    \"5078_2\"  \"Obvious tailored vehicle for Ryan Philippe. I...   \n",
       "24992   \"10069_3\"  \"<br /><br />JURASSIC PARK III *___ Adventure ...   \n",
       "24993    \"7407_8\"  \"If you're even mildly interested in the War b...   \n",
       "24994    \"7207_1\"  \"It used to be that video distributors like Su...   \n",
       "24995   \"2155_10\"  \"Sony Pictures Classics, I'm looking at you! S...   \n",
       "24996     \"59_10\"  \"I always felt that Ms. Merkerson had never go...   \n",
       "24997    \"2531_1\"  \"I was so disappointed in this movie. I am ver...   \n",
       "24998    \"7772_8\"  \"From the opening sequence, filled with black ...   \n",
       "24999  \"11465_10\"  \"This is a great horror film for people who do...   \n",
       "\n",
       "       sentiment  \n",
       "0              1  \n",
       "1              0  \n",
       "2              1  \n",
       "3              0  \n",
       "4              1  \n",
       "5              1  \n",
       "6              0  \n",
       "7              0  \n",
       "8              0  \n",
       "9              1  \n",
       "10             1  \n",
       "11             0  \n",
       "12             0  \n",
       "13             1  \n",
       "14             0  \n",
       "15             1  \n",
       "16             1  \n",
       "17             0  \n",
       "18             1  \n",
       "19             0  \n",
       "20             0  \n",
       "21             0  \n",
       "22             0  \n",
       "23             1  \n",
       "24             1  \n",
       "25             0  \n",
       "26             0  \n",
       "27             0  \n",
       "28             0  \n",
       "29             0  \n",
       "...          ...  \n",
       "24970          1  \n",
       "24971          1  \n",
       "24972          0  \n",
       "24973          0  \n",
       "24974          0  \n",
       "24975          0  \n",
       "24976          1  \n",
       "24977          0  \n",
       "24978          1  \n",
       "24979          1  \n",
       "24980          0  \n",
       "24981          1  \n",
       "24982          1  \n",
       "24983          1  \n",
       "24984          1  \n",
       "24985          0  \n",
       "24986          0  \n",
       "24987          0  \n",
       "24988          0  \n",
       "24989          1  \n",
       "24990          0  \n",
       "24991          0  \n",
       "24992          0  \n",
       "24993          1  \n",
       "24994          0  \n",
       "24995          1  \n",
       "24996          1  \n",
       "24997          1  \n",
       "24998          1  \n",
       "24999          0  \n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a random forest to the training data, using 100 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n",
    "\n",
    "forest = forest.fit( trainDataVecs, train[\"sentiment\"] )\n",
    "\n",
    "# Test & extract results \n",
    "result = forest.predict( testDataVecs )\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"],\"review\":test[\"review\"],\"sentiment\":result} )\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mas analisando o Word2Vec \n",
    "\n",
    "Os resultados foram melhores que o BoW?\n",
    "\n",
    "Se analisarmos de uma forma minuciosa, vemos que os resultados do BoW foram **um pouco melhores** que o do Word2Vec, mas porquê isso ocorre?\n",
    "\n",
    "O fato da gente utilizar a média para criar as sentenças faz com que nós percamos a ordem das palavras (além de perdermos informação ao resumir os dados por meio da média), fazendo com que a abordagem seja muito parecida com o conceito do Bag Of Words, de forma que os métodos até sejam considerados equivalentes.\n",
    "\n",
    "Algumas coisas para melhorar:\n",
    "- Dados, dados dados. Lembre-se que o paper original usou um corpus de mais de um **bilhão** de palavras enquanto os nossos dados tinham aproximadamente 18 **milhões** de palavras. É possível usar vetorer pré treinados para usar em nossos dados e talvez isso resulte em resultados mais interessantes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (somostera)",
   "language": "python",
   "name": "somostera"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
